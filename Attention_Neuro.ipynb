{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/travislatchman/Neuroimage-Registration-and-Synthesis/blob/main/Attention_Neuro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "TUuFEWLQMkqd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW0EUVHtQOqt",
        "outputId": "a55f7e06-316b-4ab0-b5f6-3a86a2169bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.1-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.2)\n",
            "Collecting nibabel>=3.2.0 (from nilearn)\n",
            "  Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nibabel, nilearn\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "Successfully installed nibabel-5.1.0 nilearn-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.22.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel) (23.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn\n",
        "!pip install nibabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBYSQ831xUwZ",
        "outputId": "f21bfe56-f291-47af-d0ca-a085e0ed844c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=cde83b6031ea5064b7c485d0f4050508628d50930d674da092c650d09a7ab6ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpXCwqlIqWP5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import mean_squared_error as mse, peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
        "import csv\n",
        "from torchviz import make_dot\n",
        "import torch.onnx\n",
        "import random\n",
        "import time\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SsbKTHpQLMU",
        "outputId": "0b56430c-967c-4c0d-98bd-f3c91ba4df90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_eHHYokbqGI",
        "outputId": "3b7062f3-eb45-444e-b2b5-668953a861a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa144416d90>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR_ahDk4sLBi"
      },
      "source": [
        "## MedIA_Project2_registered folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "446OARZCrVbz"
      },
      "outputs": [],
      "source": [
        "class MedIARegisteredDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A class used to represent a Medical Image Analysis (MedIA) Registered Dataset.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    root_dir : str\n",
        "        The path to the root directory containing the patient folders.\n",
        "    transform : callable, optional\n",
        "        An optional transform function to apply to the images.\n",
        "    patient_ids : list\n",
        "        A list of patient IDs used to identify the corresponding patient folders.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(self, root_dir, patient_ids, transform=None):\n",
        "        Initializes the MedIARegisteredDataset object.\n",
        "    __len__(self):\n",
        "        Returns the number of patients in the dataset.\n",
        "    __getitem__(self, idx):\n",
        "        Loads and returns a tuple of the input images (T1w and T2w) and target images (FA and ADC) for the patient\n",
        "        at the given index.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, patient_ids, transform=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        root_dir : str\n",
        "            The path to the root directory containing the patient folders.\n",
        "        patient_ids : list\n",
        "            A list of patient IDs used to identify the corresponding patient folders.\n",
        "        transform : callable, optional\n",
        "            An optional transform function to apply to the images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.patient_ids = patient_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Determines the number of patients in the dataset.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The number of patients in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and returns a tuple of the input images (T1w and T2w) and target images (FA and ADC) for the patient\n",
        "        at the given index.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            The index of the patient.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            A tuple containing two torch.Tensor objects:\n",
        "            - input_imgs: a torch.Tensor containing the stacked T1w and T2w images.\n",
        "            - target_imgs: a torch.Tensor containing the stacked FA and ADC images.\n",
        "        \"\"\"\n",
        "        patient_id = self.patient_ids[idx]\n",
        "        patient_folder = os.path.join(self.root_dir, patient_id)\n",
        "\n",
        "        # Load the input images (T1w and T2w)\n",
        "        t1w_path = os.path.join(patient_folder, 'T1w_1mm.nii.gz')\n",
        "        t2w_path = os.path.join(patient_folder, 'T2w_1mm_noalign.nii.gz')\n",
        "        t1w_img = nib.load(t1w_path).get_fdata()\n",
        "        t2w_img = nib.load(t2w_path).get_fdata()\n",
        "\n",
        "        # Stack the input images along the channel dimension\n",
        "        input_imgs = np.stack([t1w_img, t2w_img], axis=0)\n",
        "\n",
        "        # Load the target images (FA and ADC)\n",
        "        fa_path = os.path.join(patient_folder, 'FA_deformed.nii.gz')\n",
        "        adc_path = os.path.join(patient_folder, 'ADC_deformed.nii.gz')\n",
        "        fa_img = nib.load(fa_path).get_fdata()\n",
        "        adc_img = nib.load(adc_path).get_fdata()\n",
        "\n",
        "        # Stack the target images along the channel dimension\n",
        "        target_imgs = np.stack([fa_img, adc_img], axis=0)\n",
        "\n",
        "        if self.transform:\n",
        "            input_imgs, target_imgs = self.transform(input_imgs, target_imgs)\n",
        "\n",
        "        return torch.tensor(input_imgs).float(), torch.tensor(target_imgs).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOcyu8aozqIE"
      },
      "outputs": [],
      "source": [
        "#root_dir = r\"G:\\My Drive\\Project 2 Neuroimage Synthesis\\MedIA_Project2_registered\"\n",
        "root_dir = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/MedIA_Project2_registered\"\n",
        "\n",
        "# Define patient_ids here\n",
        "patient_ids = [f\"{i:03d}\" for i in range(1, 201) if i != 163]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f1s7ZBsnN2"
      },
      "source": [
        "## split the patient IDs into training, validation, and testing sets (80/10/10 split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRWB5h_issZt"
      },
      "outputs": [],
      "source": [
        "# #patient_ids = [str(i).zfill(3) for i in range(1, 201) if i != 163]\n",
        "rng = np.random.default_rng(seed)\n",
        "rng.shuffle(patient_ids)\n",
        "\n",
        "train_size = int(0.8 * len(patient_ids))\n",
        "val_size = int(0.1 * len(patient_ids))\n",
        "test_size = len(patient_ids) - train_size - val_size\n",
        "\n",
        "train_ids = patient_ids[:train_size]\n",
        "val_ids = patient_ids[train_size:train_size+val_size]\n",
        "test_ids = patient_ids[train_size+val_size:]\n",
        "\n",
        "# separate instances of the dataset for each split\n",
        "train_dataset = MedIARegisteredDataset(root_dir, patient_ids=train_ids)\n",
        "val_dataset = MedIARegisteredDataset(root_dir, patient_ids=val_ids)\n",
        "test_dataset = MedIARegisteredDataset(root_dir, patient_ids=test_ids)\n",
        "\n",
        "# create data loaders for each dataset\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YErk6lRVqiZk"
      },
      "source": [
        "## U-Net with Local Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJAJhYAD_tcU"
      },
      "outputs": [],
      "source": [
        "class LocalAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A class used to represent a Local Attention mechanism.\n",
        "\n",
        "    Local Attention computes the dot product of a query and a key to\n",
        "    determine the weights for a value in a locally-constrained neighborhood.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    conv_W : torch.nn.Module\n",
        "        A convolutional layer with kernel size k_size used to compute the query and the key.\n",
        "    softmax : torch.nn.Module\n",
        "        The softmax function used to compute the attention weights.\n",
        "    channels : int\n",
        "        The number of channels in the input tensor.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(self, channels, k_size=3):\n",
        "        Initializes the LocalAttention object.\n",
        "    forward(self, x):\n",
        "        Performs the forward pass of the local attention mechanism.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, k_size=3):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        channels : int\n",
        "            The number of channels in the input tensor.\n",
        "        k_size : int, optional\n",
        "            The size of the kernel for the convolution operation (default is 3).\n",
        "        \"\"\"\n",
        "        super(LocalAttention, self).__init__()\n",
        "        self.conv_W = nn.Conv3d(channels, channels, kernel_size=k_size, padding=(k_size - 1) // 2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.channels = channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the local attention mechanism.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            The input tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The output tensor after applying local attention.\n",
        "        \"\"\"\n",
        "        B, C, H, W, D = x.size()\n",
        "\n",
        "        query = self.conv_W(x).view(B, self.channels, -1)\n",
        "        key = self.conv_W(x).view(B, self.channels, -1)\n",
        "        value = x.view(B, self.channels, -1)\n",
        "\n",
        "        attention_weights = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention_weights = self.softmax(attention_weights)\n",
        "\n",
        "        out = torch.bmm(value, attention_weights)\n",
        "        out = out.view(B, self.channels, H, W, D)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model instance\n",
        "model = LocalAttention(channels=16)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "x = torch.randn(1, 16, 64, 64, 64)\n",
        "\n",
        "# Export the model to an ONNX file\n",
        "torch.onnx.export(model, x, \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/local_attention_block.onnx\")\n"
      ],
      "metadata": {
        "id": "zB7QOeycUh4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model instance\n",
        "model = LocalAttention(channels=16)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "x = torch.randn(1, 16, 64, 64, 64)\n",
        "\n",
        "# Export the model to an ONNX file\n",
        "torch.onnx.export(model, x, \"local_attention.onnx\")\n"
      ],
      "metadata": {
        "id": "B8tQoxQnUFEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_zD42Lu_tcU"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A class used to represent a U-Net model for biomedical image segmentation.\n",
        "\n",
        "    The U-Net model consists of an encoder (downsampling path), a bottleneck, and a decoder (upsampling path).\n",
        "    This particular implementation includes a LocalAttention module after the bottleneck layer.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    enc1 : torch.nn.Module\n",
        "        The first layer of the encoder.\n",
        "    enc2 : torch.nn.Module\n",
        "        The second layer of the encoder.\n",
        "    enc3 : torch.nn.Module\n",
        "        The third layer of the encoder.\n",
        "    enc4 : torch.nn.Module\n",
        "        The fourth layer of the encoder.\n",
        "    pool : torch.nn.Module\n",
        "        The MaxPool3d module used for downsampling in the encoder path.\n",
        "    middle : torch.nn.Module\n",
        "        The bottleneck convolutional layer.\n",
        "    attention : LocalAttention\n",
        "        The LocalAttention module used after the bottleneck layer.\n",
        "    up4, dec4, up3, dec3, up2, dec2, up1, dec1 : torch.nn.Module\n",
        "        The layers of the decoder.\n",
        "    output : torch.nn.Module\n",
        "        The final output layer.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(self, in_channels, out_channels):\n",
        "        Initializes the UNet object.\n",
        "    forward(self, x):\n",
        "        Performs the forward pass of the U-Net.\n",
        "    up_concat(self, up, encode, decode):\n",
        "        Performs the upsampling and concatenation step in the decoder path.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channels : int\n",
        "            The number of channels in the input tensor.\n",
        "        out_channels : int\n",
        "            The number of channels in the output tensor (typically the number of classes).\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.enc1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
        "        self.enc2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
        "        self.enc3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
        "        self.enc4 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.middle = nn.Conv3d(256, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.attention = LocalAttention(512)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec4 = nn.Conv3d(512, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec3 = nn.Conv3d(256, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec2 = nn.Conv3d(128, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec1 = nn.Conv3d(64, 32, kernel_size=3, padding=1)\n",
        "\n",
        "        self.output = nn.Conv3d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the U-Net model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            The input tensor with shape (batch_size, in_channels, height, width, depth).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output : torch.Tensor\n",
        "            The output tensor after the forward pass through the U-Net.\n",
        "            The shape is (batch_size, out_channels, height, width, depth).\n",
        "        \"\"\"\n",
        "\n",
        "        enc1 = F.relu(self.enc1(x))\n",
        "        enc2 = F.relu(self.enc2(self.pool(enc1)))\n",
        "        enc3 = F.relu(self.enc3(self.pool(enc2)))\n",
        "        enc4 = F.relu(self.enc4(self.pool(enc3)))\n",
        "\n",
        "        middle = F.relu(self.middle(self.pool(enc4)))\n",
        "\n",
        "        # Use LocalAttention after the bottleneck layer\n",
        "        middle = self.attention(middle)\n",
        "\n",
        "        up4 = self.up_concat(self.up4, enc4, middle)\n",
        "        dec4 = F.relu(self.dec4(up4))\n",
        "\n",
        "        up3 = self.up_concat(self.up3, enc3, dec4)\n",
        "        dec3 = F.relu(self.dec3(up3))\n",
        "\n",
        "        up2 = self.up_concat(self.up2, enc2, dec3)\n",
        "        dec2 = F.relu(self.dec2(up2))\n",
        "\n",
        "        up1 = self.up_concat(self.up1, enc1, dec2)\n",
        "        dec1 = F.relu(self.dec1(up1))\n",
        "\n",
        "        output = self.output(dec1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def up_concat(self, up, encode, decode):\n",
        "        \"\"\"\n",
        "        Performs the upsampling and concatenation step in the decoder path of the U-Net.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        up : torch.nn.Module\n",
        "            The ConvTranspose3d layer used for upsampling.\n",
        "        encode : torch.Tensor\n",
        "            The output tensor from the corresponding encoder layer.\n",
        "        decode : torch.Tensor\n",
        "            The output tensor from the previous layer in the decoder path.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        merge : torch.Tensor\n",
        "            The output tensor after upsampling and concatenation.\n",
        "        \"\"\"\n",
        "\n",
        "        up = up(decode)\n",
        "        up = F.interpolate(up, size=encode.size()[2:], mode='trilinear', align_corners=False)\n",
        "        merge = torch.cat([encode, up], dim=1)\n",
        "        return merge\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "\n",
        "# Define the LocalAttention class here...\n",
        "\n",
        "# Define the UNet class here...\n",
        "\n",
        "# Create a model instance\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "x = torch.randn(1, 3, 32, 32, 32)\n",
        "\n",
        "# Export the model to an ONNX file\n",
        "torch.onnx.export(model, x, \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/unet_local_attention.onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpfygWEGTuBb",
        "outputId": "07673a23-828b-494f-b73b-fd00ccfbf43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPHeeXiPt9dZ"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u5KlE7Ydc8P"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model\n",
        "model = UNet(2, 2).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "patience = 10\n",
        "early_stopping_counter = 0\n",
        "#model_save_path = \"model.pt\"\n",
        "# model_save_path = r\"G:\\My Drive\\Project 2 Neuroimage Synthesis\\model.pt\"\n",
        "model_save_path = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/attention.pt\"\n",
        "best_validation_loss = float('inf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DePsGVsYt_PE",
        "outputId": "97960d06-2125-4cc5-9f93-2cb17af32897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 4.5607, Val Loss: 1.0491, Epoch Duration: 739.3728 seconds, Total Time: 739.3733 seconds\n",
            "Epoch 2/100, Train Loss: 0.5488, Val Loss: 0.0719, Epoch Duration: 133.6278 seconds, Total Time: 873.1539 seconds\n",
            "Epoch 3/100, Train Loss: 0.0339, Val Loss: 0.0318, Epoch Duration: 133.3235 seconds, Total Time: 1006.6436 seconds\n",
            "Epoch 4/100, Train Loss: 0.0307, Val Loss: 0.0299, Epoch Duration: 133.6598 seconds, Total Time: 1140.4674 seconds\n",
            "Epoch 5/100, Train Loss: 0.0284, Val Loss: 0.0277, Epoch Duration: 131.0768 seconds, Total Time: 1271.7120 seconds\n",
            "Epoch 6/100, Train Loss: 0.0266, Val Loss: 0.0263, Epoch Duration: 130.6803 seconds, Total Time: 1402.5601 seconds\n",
            "Epoch 7/100, Train Loss: 0.0256, Val Loss: 0.0255, Epoch Duration: 131.4874 seconds, Total Time: 1534.2173 seconds\n",
            "Epoch 8/100, Train Loss: 0.0248, Val Loss: 0.0245, Epoch Duration: 134.3501 seconds, Total Time: 1668.7471 seconds\n",
            "Epoch 9/100, Train Loss: 0.0236, Val Loss: 0.0234, Epoch Duration: 134.0047 seconds, Total Time: 1802.9246 seconds\n",
            "Epoch 10/100, Train Loss: 0.0228, Val Loss: 0.0228, Epoch Duration: 134.4163 seconds, Total Time: 1937.5052 seconds\n",
            "Epoch 11/100, Train Loss: 0.0223, Val Loss: 0.0224, Epoch Duration: 128.8493 seconds, Total Time: 2066.5251 seconds\n",
            "Epoch 12/100, Train Loss: 0.0220, Val Loss: 0.0221, Epoch Duration: 134.8706 seconds, Total Time: 2201.5670 seconds\n",
            "Epoch 13/100, Train Loss: 0.0216, Val Loss: 0.0217, Epoch Duration: 131.1373 seconds, Total Time: 2332.8736 seconds\n",
            "Epoch 14/100, Train Loss: 0.0213, Val Loss: 0.0214, Epoch Duration: 128.6718 seconds, Total Time: 2461.7168 seconds\n",
            "Epoch 15/100, Train Loss: 0.0209, Val Loss: 0.0210, Epoch Duration: 134.3115 seconds, Total Time: 2596.1975 seconds\n",
            "Epoch 16/100, Train Loss: 0.0206, Val Loss: 0.0207, Epoch Duration: 134.8859 seconds, Total Time: 2731.2616 seconds\n",
            "Epoch 17/100, Train Loss: 0.0202, Val Loss: 0.0203, Epoch Duration: 131.0785 seconds, Total Time: 2862.5028 seconds\n",
            "Epoch 18/100, Train Loss: 0.0199, Val Loss: 0.0199, Epoch Duration: 134.1961 seconds, Total Time: 2996.8702 seconds\n",
            "Epoch 19/100, Train Loss: 0.0195, Val Loss: 0.0196, Epoch Duration: 133.8549 seconds, Total Time: 3130.9018 seconds\n",
            "Epoch 20/100, Train Loss: 0.0192, Val Loss: 0.0192, Epoch Duration: 134.4036 seconds, Total Time: 3265.4847 seconds\n",
            "Epoch 21/100, Train Loss: 0.0188, Val Loss: 0.0189, Epoch Duration: 137.3459 seconds, Total Time: 3402.9993 seconds\n",
            "Epoch 22/100, Train Loss: 0.0185, Val Loss: 0.0185, Epoch Duration: 134.1070 seconds, Total Time: 3537.2753 seconds\n",
            "Epoch 23/100, Train Loss: 0.0181, Val Loss: 0.0182, Epoch Duration: 134.6978 seconds, Total Time: 3672.1534 seconds\n",
            "Epoch 24/100, Train Loss: 0.0178, Val Loss: 0.0178, Epoch Duration: 131.8761 seconds, Total Time: 3804.1975 seconds\n",
            "Epoch 25/100, Train Loss: 0.0174, Val Loss: 0.0174, Epoch Duration: 134.7470 seconds, Total Time: 3939.1176 seconds\n",
            "Epoch 26/100, Train Loss: 0.0170, Val Loss: 0.0171, Epoch Duration: 135.3415 seconds, Total Time: 4074.6315 seconds\n",
            "Epoch 27/100, Train Loss: 0.0167, Val Loss: 0.0167, Epoch Duration: 134.4840 seconds, Total Time: 4209.2891 seconds\n",
            "Epoch 28/100, Train Loss: 0.0163, Val Loss: 0.0164, Epoch Duration: 135.1938 seconds, Total Time: 4344.6598 seconds\n",
            "Epoch 29/100, Train Loss: 0.0160, Val Loss: 0.0160, Epoch Duration: 135.2039 seconds, Total Time: 4480.0368 seconds\n",
            "Epoch 30/100, Train Loss: 0.0156, Val Loss: 0.0157, Epoch Duration: 134.4984 seconds, Total Time: 4614.7131 seconds\n",
            "Epoch 31/100, Train Loss: 0.0153, Val Loss: 0.0153, Epoch Duration: 134.9330 seconds, Total Time: 4749.8204 seconds\n",
            "Epoch 32/100, Train Loss: 0.0150, Val Loss: 0.0150, Epoch Duration: 131.4616 seconds, Total Time: 4881.4577 seconds\n",
            "Epoch 33/100, Train Loss: 0.0146, Val Loss: 0.0147, Epoch Duration: 135.4197 seconds, Total Time: 5017.0439 seconds\n",
            "Epoch 34/100, Train Loss: 0.0143, Val Loss: 0.0143, Epoch Duration: 135.3731 seconds, Total Time: 5152.5889 seconds\n",
            "Epoch 35/100, Train Loss: 0.0140, Val Loss: 0.0140, Epoch Duration: 131.7864 seconds, Total Time: 5284.5464 seconds\n",
            "Epoch 36/100, Train Loss: 0.0137, Val Loss: 0.0137, Epoch Duration: 135.9138 seconds, Total Time: 5420.6375 seconds\n",
            "Epoch 37/100, Train Loss: 0.0134, Val Loss: 0.0134, Epoch Duration: 135.4490 seconds, Total Time: 5556.2605 seconds\n",
            "Epoch 38/100, Train Loss: 0.0131, Val Loss: 0.0131, Epoch Duration: 134.4379 seconds, Total Time: 5690.8814 seconds\n",
            "Epoch 39/100, Train Loss: 0.0128, Val Loss: 0.0128, Epoch Duration: 135.0175 seconds, Total Time: 5826.0811 seconds\n",
            "Epoch 40/100, Train Loss: 0.0125, Val Loss: 0.0125, Epoch Duration: 134.9338 seconds, Total Time: 5961.1868 seconds\n",
            "Epoch 41/100, Train Loss: 0.0122, Val Loss: 0.0122, Epoch Duration: 134.9689 seconds, Total Time: 6096.3327 seconds\n",
            "Epoch 42/100, Train Loss: 0.0119, Val Loss: 0.0120, Epoch Duration: 135.2402 seconds, Total Time: 6231.7453 seconds\n",
            "Epoch 43/100, Train Loss: 0.0117, Val Loss: 0.0117, Epoch Duration: 135.2008 seconds, Total Time: 6367.1183 seconds\n",
            "Epoch 44/100, Train Loss: 0.0114, Val Loss: 0.0114, Epoch Duration: 135.4359 seconds, Total Time: 6502.7289 seconds\n",
            "Epoch 45/100, Train Loss: 0.0111, Val Loss: 0.0111, Epoch Duration: 132.2587 seconds, Total Time: 6635.1604 seconds\n",
            "Epoch 46/100, Train Loss: 0.0109, Val Loss: 0.0109, Epoch Duration: 131.4023 seconds, Total Time: 6766.7265 seconds\n",
            "Epoch 47/100, Train Loss: 0.0107, Val Loss: 0.0107, Epoch Duration: 135.2581 seconds, Total Time: 6902.1592 seconds\n",
            "Epoch 48/100, Train Loss: 0.0104, Val Loss: 0.0104, Epoch Duration: 134.7480 seconds, Total Time: 7037.0783 seconds\n",
            "Epoch 49/100, Train Loss: 0.0102, Val Loss: 0.0102, Epoch Duration: 134.5609 seconds, Total Time: 7171.8115 seconds\n",
            "Epoch 50/100, Train Loss: 0.0100, Val Loss: 0.0100, Epoch Duration: 134.9486 seconds, Total Time: 7306.9311 seconds\n",
            "Epoch 51/100, Train Loss: 0.0097, Val Loss: 0.0097, Epoch Duration: 134.3750 seconds, Total Time: 7441.4676 seconds\n",
            "Epoch 52/100, Train Loss: 0.0095, Val Loss: 0.0095, Epoch Duration: 134.6259 seconds, Total Time: 7576.2611 seconds\n",
            "Epoch 53/100, Train Loss: 0.0093, Val Loss: 0.0093, Epoch Duration: 131.6966 seconds, Total Time: 7708.1329 seconds\n",
            "Epoch 54/100, Train Loss: 0.0091, Val Loss: 0.0091, Epoch Duration: 134.8066 seconds, Total Time: 7843.1124 seconds\n",
            "Epoch 55/100, Train Loss: 0.0089, Val Loss: 0.0089, Epoch Duration: 134.7970 seconds, Total Time: 7978.0809 seconds\n",
            "Epoch 56/100, Train Loss: 0.0087, Val Loss: 0.0087, Epoch Duration: 134.4019 seconds, Total Time: 8112.6584 seconds\n",
            "Epoch 57/100, Train Loss: 0.0085, Val Loss: 0.0085, Epoch Duration: 131.9143 seconds, Total Time: 8244.7530 seconds\n",
            "Epoch 58/100, Train Loss: 0.0083, Val Loss: 0.0084, Epoch Duration: 138.7468 seconds, Total Time: 8383.6718 seconds\n",
            "Epoch 59/100, Train Loss: 0.0081, Val Loss: 0.0081, Epoch Duration: 131.9679 seconds, Total Time: 8515.8302 seconds\n",
            "Epoch 60/100, Train Loss: 0.0080, Val Loss: 0.0079, Epoch Duration: 134.7963 seconds, Total Time: 8650.7975 seconds\n",
            "Epoch 61/100, Train Loss: 0.0078, Val Loss: 0.0078, Epoch Duration: 135.2398 seconds, Total Time: 8786.2065 seconds\n",
            "Epoch 62/100, Train Loss: 0.0076, Val Loss: 0.0076, Epoch Duration: 131.7668 seconds, Total Time: 8918.1405 seconds\n",
            "Epoch 63/100, Train Loss: 0.0075, Val Loss: 0.0075, Epoch Duration: 135.9822 seconds, Total Time: 9054.2989 seconds\n",
            "Epoch 64/100, Train Loss: 0.0073, Val Loss: 0.0073, Epoch Duration: 132.3830 seconds, Total Time: 9186.8673 seconds\n",
            "Epoch 65/100, Train Loss: 0.0071, Val Loss: 0.0071, Epoch Duration: 135.5181 seconds, Total Time: 9322.5629 seconds\n",
            "Epoch 66/100, Train Loss: 0.0070, Val Loss: 0.0070, Epoch Duration: 131.6931 seconds, Total Time: 9454.4216 seconds\n",
            "Epoch 67/100, Train Loss: 0.0068, Val Loss: 0.0069, Epoch Duration: 136.4178 seconds, Total Time: 9591.0127 seconds\n",
            "Epoch 68/100, Train Loss: 0.0067, Val Loss: 0.0067, Epoch Duration: 136.0571 seconds, Total Time: 9727.2433 seconds\n",
            "Epoch 69/100, Train Loss: 0.0066, Val Loss: 0.0066, Epoch Duration: 135.4464 seconds, Total Time: 9862.8664 seconds\n",
            "Epoch 70/100, Train Loss: 0.0064, Val Loss: 0.0064, Epoch Duration: 135.7917 seconds, Total Time: 9998.8305 seconds\n",
            "Epoch 71/100, Train Loss: 0.0062, Val Loss: 0.0062, Epoch Duration: 134.5459 seconds, Total Time: 10133.5430 seconds\n",
            "Epoch 72/100, Train Loss: 0.0060, Val Loss: 0.0060, Epoch Duration: 135.4940 seconds, Total Time: 10269.2249 seconds\n",
            "Epoch 73/100, Train Loss: 0.0058, Val Loss: 0.0059, Epoch Duration: 134.8753 seconds, Total Time: 10404.2811 seconds\n",
            "Epoch 74/100, Train Loss: 0.0057, Val Loss: 0.0057, Epoch Duration: 135.3452 seconds, Total Time: 10539.7997 seconds\n",
            "Epoch 75/100, Train Loss: 0.0056, Val Loss: 0.0056, Epoch Duration: 132.1705 seconds, Total Time: 10672.1446 seconds\n",
            "Epoch 76/100, Train Loss: 0.0054, Val Loss: 0.0055, Epoch Duration: 134.3312 seconds, Total Time: 10806.6461 seconds\n",
            "Epoch 77/100, Train Loss: 0.0053, Val Loss: 0.0053, Epoch Duration: 135.8781 seconds, Total Time: 10942.6928 seconds\n",
            "Epoch 78/100, Train Loss: 0.0051, Val Loss: 0.0051, Epoch Duration: 135.0280 seconds, Total Time: 11077.8977 seconds\n",
            "Epoch 79/100, Train Loss: 0.0049, Val Loss: 0.0050, Epoch Duration: 135.9721 seconds, Total Time: 11214.0704 seconds\n",
            "Epoch 80/100, Train Loss: 0.0048, Val Loss: 0.0048, Epoch Duration: 135.8452 seconds, Total Time: 11350.0965 seconds\n",
            "Epoch 81/100, Train Loss: 0.0047, Val Loss: 0.0047, Epoch Duration: 135.3731 seconds, Total Time: 11485.6440 seconds\n",
            "Epoch 82/100, Train Loss: 0.0046, Val Loss: 0.0046, Epoch Duration: 135.9780 seconds, Total Time: 11621.7974 seconds\n",
            "Epoch 83/100, Train Loss: 0.0045, Val Loss: 0.0045, Epoch Duration: 134.6851 seconds, Total Time: 11756.6644 seconds\n",
            "Epoch 84/100, Train Loss: 0.0044, Val Loss: 0.0044, Epoch Duration: 134.6517 seconds, Total Time: 11891.4990 seconds\n",
            "Epoch 85/100, Train Loss: 0.0043, Val Loss: 0.0043, Epoch Duration: 133.9763 seconds, Total Time: 12025.6470 seconds\n",
            "Epoch 86/100, Train Loss: 0.0042, Val Loss: 0.0042, Epoch Duration: 134.6176 seconds, Total Time: 12160.4357 seconds\n",
            "Epoch 87/100, Train Loss: 0.0041, Val Loss: 0.0041, Epoch Duration: 135.4376 seconds, Total Time: 12296.0520 seconds\n",
            "Epoch 88/100, Train Loss: 0.0040, Val Loss: 0.0041, Epoch Duration: 134.3293 seconds, Total Time: 12430.5572 seconds\n",
            "Epoch 89/100, Train Loss: 0.0040, Val Loss: 0.0042, Epoch Duration: 135.0407 seconds, Total Time: 12565.7681 seconds\n",
            "Epoch 90/100, Train Loss: 0.0039, Val Loss: 0.0039, Epoch Duration: 131.8434 seconds, Total Time: 12697.6117 seconds\n",
            "Epoch 91/100, Train Loss: 0.0038, Val Loss: 0.0038, Epoch Duration: 131.7042 seconds, Total Time: 12829.4928 seconds\n",
            "Epoch 92/100, Train Loss: 0.0037, Val Loss: 0.0039, Epoch Duration: 131.6238 seconds, Total Time: 12961.2900 seconds\n",
            "Epoch 93/100, Train Loss: 0.0036, Val Loss: 0.0037, Epoch Duration: 128.2645 seconds, Total Time: 13089.5547 seconds\n",
            "Epoch 94/100, Train Loss: 0.0036, Val Loss: 0.0037, Epoch Duration: 131.2637 seconds, Total Time: 13220.9952 seconds\n",
            "Epoch 95/100, Train Loss: 0.0035, Val Loss: 0.0035, Epoch Duration: 134.1794 seconds, Total Time: 13355.1748 seconds\n",
            "Epoch 96/100, Train Loss: 0.0034, Val Loss: 0.0036, Epoch Duration: 134.7589 seconds, Total Time: 13490.1049 seconds\n",
            "Epoch 97/100, Train Loss: 0.0034, Val Loss: 0.0034, Epoch Duration: 133.8349 seconds, Total Time: 13623.9401 seconds\n",
            "Epoch 98/100, Train Loss: 0.0033, Val Loss: 0.0034, Epoch Duration: 130.2940 seconds, Total Time: 13754.4141 seconds\n",
            "Epoch 99/100, Train Loss: 0.0033, Val Loss: 0.0033, Epoch Duration: 133.1412 seconds, Total Time: 13887.5559 seconds\n",
            "Epoch 100/100, Train Loss: 0.0032, Val Loss: 0.0033, Epoch Duration: 130.7401 seconds, Total Time: 14018.4617 seconds\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "patience = 10\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    model.train()\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for i, (input_imgs, target_imgs) in enumerate(train_loader):\n",
        "        input_imgs = input_imgs.to(device)\n",
        "        target_imgs = F.interpolate(target_imgs, size=[182, 218, 182], mode='trilinear', align_corners=False)\n",
        "        #target_imgs = F.interpolate(target_imgs, size=(182, 218, 182), mode='trilinear', align_corners=False)\n",
        "        target_imgs = target_imgs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_imgs)\n",
        "        loss = criterion(outputs, target_imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * input_imgs.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = F.interpolate(targets, size=[182, 218, 182], mode='trilinear', align_corners=False)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    total_training_time = time.time() - start_time\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch Duration: {epoch_duration:.4f} seconds, Total Time: {total_training_time:.4f} seconds')\n",
        "\n",
        "    if val_loss < best_validation_loss:\n",
        "        best_validation_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myYHyL3-m5ou",
        "outputId": "04359d8f-c7e8-44e9-9aea-741e72646587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training time: 20745.4674 seconds\n",
            "Average Epoch Time: 103.73 seconds\n"
          ]
        }
      ],
      "source": [
        "# Print total training time\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f'Total training time: {total_time:.4f} seconds')\n",
        "\n",
        "avg_epoch_time = total_time / num_epochs\n",
        "print(f\"Average Epoch Time: {avg_epoch_time:.2f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model State"
      ],
      "metadata": {
        "id": "9eyWefb0Pyvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0T9eVvwkY4n"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, \"attention.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMcgQG-dzPZV"
      },
      "source": [
        "## Train/Val Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "OBeNThFdpoUd",
        "outputId": "c5797ac0-d575-4649-bbeb-344ef98d92af"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAHACAYAAAB3WSN5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOoklEQVR4nO3deXxU9b3/8fc5Z5LJnhAgmwQEZROBKiJF3KECWgTBqpRWsLZURVpcWi9WEbEtXm2V64Ztr4LWrerPra4FVFxAVBQpLlQQWS4EEMhOkpk5398fkxkSWQ3JnEN4PR+PeZA5c2bymZOTcN7z/Z7PsYwxRgAAAAAASZLtdQEAAAAA4CeEJAAAAABogJAEAAAAAA0QkgAAAACgAUISAAAAADRASAIAAACABghJAAAAANAAIQkAAAAAGgh4XUBLc11XGzduVGZmpizL8rocAAAAAB4xxqiiokJFRUWy7b2PF7X6kLRx40YVFxd7XQYAAAAAn1i/fr06dOiw18dbfUjKzMyUFN0QWVlZHlcDAAAAwCvl5eUqLi6OZ4S9afUhKTbFLisri5AEAAAAYL+n4dC4AQAAAAAaICQBAAAAQAOEJAAAAABooNWfkwQAAADsizFG4XBYkUjE61JwkBzHUSAQOOhL/xCSAAAAcNiqq6vTpk2bVF1d7XUpaCZpaWkqLCxUcnJyk1+DkAQAAIDDkuu6WrNmjRzHUVFRkZKTkw96BALeMcaorq5OW7du1Zo1a9S1a9d9XjB2XwhJAAAAOCzV1dXJdV0VFxcrLS3N63LQDFJTU5WUlKS1a9eqrq5OKSkpTXodGjcAAADgsNbU0Qb4U3P8PNkjAAAAAKABQhIAAAAANEBIAgAAAA5zRx55pGbNmuV1Gb5BSAIAAAAOEZZl7fM2ffr0Jr3uBx98oIkTJx5UbaeffrqmTJlyUK/hF3S3AwAAAA4RmzZtin/9j3/8Q9OmTdPKlSvjyzIyMuJfG2MUiUQUCOz/kL99+/bNW+ghjpGkBLrwL4t11p0LtXZbldelAAAA4FuMMaquC3tyM8YcUI0FBQXxW3Z2tizLit//4osvlJmZqVdeeUX9+vVTMBjUO++8o9WrV2vkyJHKz89XRkaG+vfvr/nz5zd63W9Pt7MsS//7v/+r8847T2lpaeratateeOGFg9q+/+///T/16tVLwWBQRx55pP785z83evy+++5T165dlZKSovz8fJ1//vnxx55++mn17t1bqampatu2rYYMGaKqqpY7pmYkKYFWb63UN5V12hmKeF0KAAAAvmVnKKJjpr3myff+bMZQpSU3z6H5f/3Xf+lPf/qTunTpojZt2mj9+vU6++yz9Yc//EHBYFAPP/ywRowYoZUrV6pjx457fZ2bb75Zt912m26//XbdfffdGjdunNauXavc3NzvXNPSpUt1wQUXaPr06brwwgu1aNEiXXHFFWrbtq0mTJigDz/8UL/61a/097//XSeddJK2b9+ut99+W1J09Gzs2LG67bbbdN5556miokJvv/32AQfLpiAkJZBdfwXniNtyP1AAAAAc3mbMmKEf/OAH8fu5ubnq27dv/P4tt9yiZ599Vi+88IKuvPLKvb7OhAkTNHbsWEnSH//4R9111116//33NWzYsO9c0x133KHBgwfrxhtvlCR169ZNn332mW6//XZNmDBB69atU3p6un74wx8qMzNTnTp10nHHHScpGpLC4bBGjx6tTp06SZJ69+79nWv4LghJCeTY0ZDUgqEXAAAATZSa5OizGUM9+97N5YQTTmh0v7KyUtOnT9dLL70UDxw7d+7UunXr9vk6ffr0iX+dnp6urKwsbdmypUk1ff755xo5cmSjZYMGDdKsWbMUiUT0gx/8QJ06dVKXLl00bNgwDRs2LD7Vr2/fvho8eLB69+6toUOH6qyzztL555+vNm3aNKmWA8E5SQnESBIAAIB/WZaltOSAJzer/jixOaSnpze6f+211+rZZ5/VH//4R7399ttatmyZevfurbq6un2+TlJS0m7bx3XdZquzoczMTH300Ud6/PHHVVhYqGnTpqlv374qLS2V4ziaN2+eXnnlFR1zzDG6++671b17d61Zs6ZFapEISQll12/tCENJAAAASJB3331XEyZM0HnnnafevXuroKBAX3/9dUJr6Nmzp959993d6urWrZscJzqKFggENGTIEN12221avny5vv76a73++uuSogFt0KBBuvnmm/Xxxx8rOTlZzz77bIvVy3S7BHLqPyFwGUkCAABAgnTt2lXPPPOMRowYIcuydOONN7bYiNDWrVu1bNmyRssKCwt1zTXXqH///rrlllt04YUXavHixbrnnnt03333SZJefPFFffXVVzr11FPVpk0bvfzyy3JdV927d9eSJUu0YMECnXXWWcrLy9OSJUu0detW9ezZs0Xeg+TxSNLMmTPVv39/ZWZmKi8vT6NGjWrU512KXpTq2xfJuuyyyzyq+ODYNtPtAAAAkFh33HGH2rRpo5NOOkkjRozQ0KFDdfzxx7fI93rsscd03HHHNbr97W9/0/HHH68nn3xSTzzxhI499lhNmzZNM2bM0IQJEyRJOTk5euaZZ3TmmWeqZ8+euv/++/X444+rV69eysrK0ltvvaWzzz5b3bp10w033KA///nPGj58eIu8B0myTEv2ztuPYcOG6aKLLlL//v0VDod1/fXXa8WKFfrss8/icylPP/10devWTTNmzIg/Ly0tTVlZWQf0PcrLy5Wdna2ysrIDfk5L+cEdC/Xllko99osBOumodp7WAgAAcLirqanRmjVr1LlzZ6WkpHhdDprJvn6uB5oNPJ1u9+qrrza6P3fuXOXl5Wnp0qU69dRT48vT0tJUUFCQ6PKaXay7XQuNbgIAAABoBr5q3FBWViZJu12g6tFHH1W7du107LHHaurUqaqurt7ra9TW1qq8vLzRzS9i3e1cGjcAAAAAvuWbxg2u62rKlCkaNGiQjj322PjyH//4x+rUqZOKioq0fPlyXXfddVq5cqWeeeaZPb7OzJkzdfPNNyeq7O+E7nYAAACA//kmJE2aNEkrVqzQO++802j5xIkT41/37t1bhYWFGjx4sFavXq2jjjpqt9eZOnWqrr766vj98vJyFRcXt1zh3wHd7QAAAAD/80VIuvLKK/Xiiy/qrbfeUocOHfa57oABAyRJq1at2mNICgaDCgaDLVLnwaK7HQAAAOB/noYkY4wmT56sZ599Vm+++aY6d+683+fE+q4XFha2cHXNz+GcJAAAAMD3PA1JkyZN0mOPPabnn39emZmZKikpkSRlZ2crNTVVq1ev1mOPPaazzz5bbdu21fLly3XVVVfp1FNPVZ8+fbwsvUl2jSR5XAgAAACAvfI0JM2ePVtS9FpIDc2ZM0cTJkxQcnKy5s+fr1mzZqmqqkrFxcUaM2aMbrjhBg+qPXixkSQaNwAAAAD+5fl0u30pLi7WwoULE1RNy9t1nSRCEgAAALxz+umn63vf+55mzZrldSm+5KvrJLV29QNJnJMEAACAJhkxYoSGDRu2x8fefvttWZal5cuXH/T3mTt3rnJycg76dQ5VhKQEcuhuBwAAgINw6aWXat68edqwYcNuj82ZM0cnnHDCIXnuvt8QkhKI7nYAAAA4GD/84Q/Vvn17zZ07t9HyyspKPfXUU7r00ku1bds2jR07VkcccYTS0tLUu3dvPf74481ax7p16zRy5EhlZGQoKytLF1xwgTZv3hx//JNPPtEZZ5yhzMxMZWVlqV+/fvrwww8lSWvXrtWIESPUpk0bpaenq1evXnr55Zebtb6D5YvrJB0u6G4HAADgY8ZIoWpvvndS2q5zM/YhEAjo4osv1ty5c/W73/1OVv1znnrqKUUiEY0dO1aVlZXq16+frrvuOmVlZemll17ST3/6Ux111FE68cQTD7pU13XjAWnhwoUKh8OaNGmSLrzwQr355puSpHHjxum4447T7Nmz5TiOli1bpqSkJEnRDtd1dXV66623lJ6ers8++0wZGRkHXVdzIiQlEN3tAAAAfCxULf2xyJvvff1GKTn9gFb92c9+pttvv10LFy6Md4meM2eOxowZo+zsbGVnZ+vaa6+Nrz958mS99tprevLJJ5slJC1YsED//ve/tWbNGhUXF0uSHn74YfXq1UsffPCB+vfvr3Xr1uk3v/mNevToIUnq2rVr/Pnr1q3TmDFj1Lt3b0lSly5dDrqm5sZ0uwSiux0AAAAOVo8ePXTSSSfpwQcflCStWrVKb7/9ti699FJJUiQS0S233KLevXsrNzdXGRkZeu2117Ru3bpm+f6ff/65iouL4wFJko455hjl5OTo888/lyRdffXV+vnPf64hQ4bo1ltv1erVq+Pr/upXv9Lvf/97DRo0SDfddFOzNJpobowkJZBN4wYAAAD/SkqLjuh49b2/g0svvVSTJ0/Wvffeqzlz5uioo47SaaedJkm6/fbb9T//8z+aNWuWevfurfT0dE2ZMkV1dXUtUfkeTZ8+XT/+8Y/10ksv6ZVXXtFNN92kJ554Quedd55+/vOfa+jQoXrppZf0r3/9SzNnztSf//xnTZ48OWH17Q8jSQlk0wIcAADAvywrOuXNi9sBnI/U0AUXXCDbtvXYY4/p4Ycf1s9+9rP4+UnvvvuuRo4cqZ/85Cfq27evunTpov/85z/Ntpl69uyp9evXa/369fFln332mUpLS3XMMcfEl3Xr1k1XXXWV/vWvf2n06NGaM2dO/LHi4mJddtlleuaZZ3TNNdfob3/7W7PV1xwYSUogutsBAACgOWRkZOjCCy/U1KlTVV5ergkTJsQf69q1q55++mktWrRIbdq00R133KHNmzc3CjAHIhKJaNmyZY2WBYNBDRkyRL1799a4ceM0a9YshcNhXXHFFTrttNN0wgknaOfOnfrNb36j888/X507d9aGDRv0wQcfaMyYMZKkKVOmaPjw4erWrZt27NihN954Qz179jzYTdKsCEkJRHc7AAAANJdLL71UDzzwgM4++2wVFe1qOHHDDTfoq6++0tChQ5WWlqaJEydq1KhRKisr+06vX1lZqeOOO67RsqOOOkqrVq3S888/r8mTJ+vUU0+VbdsaNmyY7r77bkmS4zjatm2bLr74Ym3evFnt2rXT6NGjdfPNN0uKhq9JkyZpw4YNysrK0rBhw3TnnXce5NZoXpYxrXtYo7y8XNnZ2SorK1NWVpantVz39HL948P1+s3Q7pp0xtGe1gIAAHC4q6mp0Zo1a9S5c2elpKR4XQ6ayb5+rgeaDTgnKYFo3AAAAAD4HyEpgZz6rU1IAgAAAPyLkJRANG4AAAAA/I+QlEAWIQkAAADwPUJSAjl0twMAAAB8j5CUQLGQxEgSAACAf7TyZs+Hneb4eRKSEsi26G4HAADgF0lJSZKk6upqjytBc4r9PGM/36bgYrIJRHc7AAAA/3AcRzk5OdqyZYskKS0tLX4OOQ49xhhVV1dry5YtysnJkeM4TX4tQlIC0d0OAADAXwoKCiQpHpRw6MvJyYn/XJuKkJRAXEwWAADAXyzLUmFhofLy8hQKhbwuBwcpKSnpoEaQYghJCWTHR5I8LgQAAACNOI7TLAfXaB1o3JBA8e52pCQAAADAtwhJCRTvbsc5SQAAAIBvEZISKNbdjpEkAAAAwL8ISQnESBIAAADgf4SkBHLobgcAAAD4HiEpgeKNGxhJAgAAAHyLkJRAsSs4u67HhQAAAADYK0JSAjmckwQAAAD4HiEpgehuBwAAAPgfISmB6G4HAAAA+B8hKYHobgcAAAD4HyEpgehuBwAAAPgfISmB4tPtGEkCAAAAfIuQlECxkERGAgAAAPyLkJRAdLcDAAAA/I+QlEB0twMAAAD8j5CUQPHGDYwkAQAAAL5FSEog22YkCQAAAPA7QlICOfHudh4XAgAAAGCvCEkJxHQ7AAAAwP8ISQlE4wYAAADA/whJCVQ/kCSXkAQAAAD4FiEpgZhuBwAAAPgfISmB6G4HAAAA+B8hKYFi3e1cutsBAAAAvkVISqDYdLsI0+0AAAAA3yIkJRDd7QAAAAD/IyQlEI0bAAAAAP8jJCUQLcABAAAA/yMkJZDNOUkAAACA7xGSEije3Y6MBAAAAPgWISmB6G4HAAAA+B8hKYG4mCwAAADgf4SkBNp1MVlCEgAAAOBXhKQEsuu3NiNJAAAAgH95GpJmzpyp/v37KzMzU3l5eRo1apRWrlzZaJ2amhpNmjRJbdu2VUZGhsaMGaPNmzd7VPHBiV1M1hjJEJQAAAAAX/I0JC1cuFCTJk3Se++9p3nz5ikUCumss85SVVVVfJ2rrrpK//znP/XUU09p4cKF2rhxo0aPHu1h1U0Xm24n0eEOAAAA8KuAl9/81VdfbXR/7ty5ysvL09KlS3XqqaeqrKxMDzzwgB577DGdeeaZkqQ5c+aoZ8+eeu+99/T973/fi7KbLNa4QYp2uHMa3AcAAADgD746J6msrEySlJubK0launSpQqGQhgwZEl+nR48e6tixoxYvXrzH16itrVV5eXmjm180DEUu0+0AAAAAX/JNSHJdV1OmTNGgQYN07LHHSpJKSkqUnJysnJycRuvm5+erpKRkj68zc+ZMZWdnx2/FxcUtXfoBazjdjmslAQAAAP7km5A0adIkrVixQk888cRBvc7UqVNVVlYWv61fv76ZKjx4doOtTYc7AAAAwJ88PScp5sorr9SLL76ot956Sx06dIgvLygoUF1dnUpLSxuNJm3evFkFBQV7fK1gMKhgMNjSJTdJo8YNjCQBAAAAvuTpSJIxRldeeaWeffZZvf766+rcuXOjx/v166ekpCQtWLAgvmzlypVat26dBg4cmOhyD5pNdzsAAADA9zwdSZo0aZIee+wxPf/888rMzIyfZ5Sdna3U1FRlZ2fr0ksv1dVXX63c3FxlZWVp8uTJGjhw4CHX2U7avbsdAAAAAP/xNCTNnj1bknT66ac3Wj5nzhxNmDBBknTnnXfKtm2NGTNGtbW1Gjp0qO67774EV9p8HNtSxDV0twMAAAB8ytOQZA4gKKSkpOjee+/Vvffem4CKWp5jWYrIMJIEAAAA+JRvutsdLmId7ghJAAAAgD8RkhIs1uGO6XYAAACAPxGSEizWvIGRJAAAAMCfCEkJZsdHkjwuBAAAAMAeEZISzLGZbgcAAAD4GSEpwWIjSUy3AwAAAPyJkJRgDt3tAAAAAF8jJCUY3e0AAAAAfyMkJRjd7QAAAAB/IyQlGI0bAAAAAH8jJCUYLcABAAAAfyMkJVj9QBLT7QAAAACfIiQlWHy6HSEJAAAA8CVCUoLFr5PEOUkAAACALxGSEsyhux0AAADga4SkBKO7HQAAAOBvhKQEi0+3cz0uBAAAAMAeEZISLNbdjpEkAAAAwJ8ISQlGdzsAAADA3whJCUZ3OwAAAMDfCEkJRnc7AAAAwN8ISQlGdzsAAADA3whJCUZ3OwAAAMDfCEkJRuMGAAAAwN8ISQkWawFO4wYAAADAnwhJCRabbsc5SQAAAIA/EZISjOl2AAAAgL8RkhLMpgU4AAAA4GuEpARz4heT9bgQAAAAAHtESEowptsBAAAA/kZISrD4dZJo3AAAAAD4EiEpweItwBlJAgAAAHyJkJRgsel2hpEkAAAAwJcISQm2q7udx4UAAAAA2CNCUoI5nJMEAAAA+BohKcHobgcAAAD4GyEpwehuBwAAAPgbISnBnPotzkgSAAAA4E+EpASLjyQRkgAAAABfIiQlWKy7HRkJAAAA8CdCUoLFutu5nJMEAAAA+BIhKcF2XSeJkAQAAAD4ESEpwbhOEgAAAOBvhKQEo7sdAAAA4G+EpARjuh0AAADgb4SkBONisgAAAIC/EZISLHZOEhkJAAAA8CdCUoIx3Q4AAADwN0JSgjnRjMR0OwAAAMCnCEkJ5tSPJNHdDgAAAPAnQlKCMd0OAAAA8DdCUoLFGje4TLcDAAAAfImQlGCMJAEAAAD+RkhKMDs+kuRxIQAAAAD2iJCUYE79Fme6HQAAAOBPhKQEi40kMd0OAAAA8CdCUoI5nJMEAAAA+JqnIemtt97SiBEjVFRUJMuy9NxzzzV6fMKECbIsq9Ft2LBh3hTbTOhuBwAAAPibpyGpqqpKffv21b333rvXdYYNG6ZNmzbFb48//ngCK2x+dLcDAAAA/C3g5TcfPny4hg8fvs91gsGgCgoKElRRy4uNJEXISAAAAIAv+f6cpDfffFN5eXnq3r27Lr/8cm3btm2f69fW1qq8vLzRzU/s+i1umG4HAAAA+JKvQ9KwYcP08MMPa8GCBfrv//5vLVy4UMOHD1ckEtnrc2bOnKns7Oz4rbi4OIEV7x/d7QAAAAB/83S63f5cdNFF8a979+6tPn366KijjtKbb76pwYMH7/E5U6dO1dVXXx2/X15e7qugRHc7AAAAwN98PZL0bV26dFG7du20atWqva4TDAaVlZXV6OYndLcDAAAA/O2QCkkbNmzQtm3bVFhY6HUpTUZ3OwAAAMDfPJ1uV1lZ2WhUaM2aNVq2bJlyc3OVm5urm2++WWPGjFFBQYFWr16t3/72tzr66KM1dOhQD6s+OLHpdmQkAAAAwJ88DUkffvihzjjjjPj92LlE48eP1+zZs7V8+XI99NBDKi0tVVFRkc466yzdcsstCgaDXpV80GjcAAAAAPibpyHp9NNP32cr7Ndeey2B1SRG/UASIQkAAADwqUPqnKTWIDbdjuskAQAAAP5ESEqw+HQ7QhIAAADgS4SkBNt1nSSPCwEAAACwR4SkBNvV3Y6RJAAAAMCPCEkJRnc7AAAAwN8ISQkWH0kiJAEAAAC+REhKsHgLcKbbAQAAAL5ESEqw2HQ7zkkCAAAA/ImQlGC7ptt5XAgAAACAPSIkJVi8BTgjSQAAAIAvEZISjO52AAAAgL81KSStX79eGzZsiN9///33NWXKFP31r39ttsJaq9hIkkSHOwAAAMCPmhSSfvzjH+uNN96QJJWUlOgHP/iB3n//ff3ud7/TjBkzmrXA1saxdoUkptwBAAAA/tOkkLRixQqdeOKJkqQnn3xSxx57rBYtWqRHH31Uc+fObc76Wh2rwRZnyh0AAADgP00KSaFQSMFgUJI0f/58nXvuuZKkHj16aNOmTc1XXSvUcCSJgSQAAADAf5oUknr16qX7779fb7/9tubNm6dhw4ZJkjZu3Ki2bds2a4GtTcNzkphuBwAAAPhPk0LSf//3f+svf/mLTj/9dI0dO1Z9+/aVJL3wwgvxaXjYM7vhOUlMtwMAAAB8J9CUJ51++un65ptvVF5erjZt2sSXT5w4UWlpac1WXGtEdzsAAADA35o0krRz507V1tbGA9LatWs1a9YsrVy5Unl5ec1aYGvTICMx3Q4AAADwoSaFpJEjR+rhhx+WJJWWlmrAgAH685//rFGjRmn27NnNWmBrY1lWPCgxkgQAAAD4T5NC0kcffaRTTjlFkvT0008rPz9fa9eu1cMPP6y77rqrWQtsjWLnJTGSBAAAAPhPk0JSdXW1MjMzJUn/+te/NHr0aNm2re9///tau3ZtsxbYGtn1Q0kMJAEAAAD+06SQdPTRR+u5557T+vXr9dprr+mss86SJG3ZskVZWVnNWmBrFLtWEtPtAAAAAP9pUkiaNm2arr32Wh155JE68cQTNXDgQEnRUaXjjjuuWQtsjWId7mgBDgAAAPhPk1qAn3/++Tr55JO1adOm+DWSJGnw4ME677zzmq241irWuIFzkgAAAAD/aVJIkqSCggIVFBRow4YNkqQOHTpwIdkDFBtJYrodAAAA4D9Nmm7nuq5mzJih7OxsderUSZ06dVJOTo5uueUWua7b3DW2OvHpdowkAQAAAL7TpJGk3/3ud3rggQd06623atCgQZKkd955R9OnT1dNTY3+8Ic/NGuRrY1lcU4SAAAA4FdNCkkPPfSQ/vd//1fnnntufFmfPn10xBFH6IorriAk7Uesux0DSQAAAID/NGm63fbt29WjR4/dlvfo0UPbt28/6KJaO7rbAQAAAP7VpJDUt29f3XPPPbstv+eee9SnT5+DLqq1s+u3OuckAQAAAP7TpOl2t912m8455xzNnz8/fo2kxYsXa/369Xr55ZebtcDWiIvJAgAAAP7VpJGk0047Tf/5z3903nnnqbS0VKWlpRo9erQ+/fRT/f3vf2/uGlsdm+l2AAAAgG81+TpJRUVFuzVo+OSTT/TAAw/or3/960EX1prFRpKYbgcAAAD4T5NGknBw7Ph0O48LAQAAALAbQpIHYtPtXEaSAAAAAN8hJHnAobsdAAAA4Fvf6Zyk0aNH7/Px0tLSg6nlsEF3OwAAAMC/vlNIys7O3u/jF1988UEVdDigux0AAADgX98pJM2ZM6el6jisxEeSmG4HAAAA+A7nJHlg10iSx4UAAAAA2A0hyQP1GYnGDQAAAIAPEZI84Ng0bgAAAAD8ipDkAZtzkgAAAADfIiR5wKG7HQAAAOBbhCQP0N0OAAAA8C9CkgfobgcAAAD4FyHJA7GRJLrbAQAAAP5DSPKAXb/V6W4HAAAA+A8hyQOx7nY0bgAAAAD8h5Dkgfh1kphuBwAAAPgOIckDdLcDAAAA/IuQ5AG62wEAAAD+RUjyACNJAAAAgH8RkjywaySJkAQAAAD4DSHJA079VickAQAAAP5DSPKAzXQ7AAAAwLc8DUlvvfWWRowYoaKiIlmWpeeee67R48YYTZs2TYWFhUpNTdWQIUP05ZdfelNsMyIkAQAAAP7laUiqqqpS3759de+99+7x8dtuu0133XWX7r//fi1ZskTp6ekaOnSoampqElxp83LobgcAAAD4VsDLbz58+HANHz58j48ZYzRr1izdcMMNGjlypCTp4YcfVn5+vp577jlddNFFiSy1WXExWQAAAMC/fHtO0po1a1RSUqIhQ4bEl2VnZ2vAgAFavHjxXp9XW1ur8vLyRje/iU23o3EDAAAA4D++DUklJSWSpPz8/EbL8/Pz44/tycyZM5WdnR2/FRcXt2idTUF3OwAAAMC/fBuSmmrq1KkqKyuL39avX+91SbvhYrIAAACAf/k2JBUUFEiSNm/e3Gj55s2b44/tSTAYVFZWVqOb31hMtwMAAAB8y7chqXPnziooKNCCBQviy8rLy7VkyRINHDjQw8oO3q7GDR4XAgAAAGA3nna3q6ys1KpVq+L316xZo2XLlik3N1cdO3bUlClT9Pvf/15du3ZV586ddeONN6qoqEijRo3yruhmEA9JpCQAAADAdzwNSR9++KHOOOOM+P2rr75akjR+/HjNnTtXv/3tb1VVVaWJEyeqtLRUJ598sl599VWlpKR4VXKziHe345wkAAAAwHc8DUmnn366zD6CgmVZmjFjhmbMmJHAqlperLsdI0kAAACA//j2nKTWjJEkAAAAwL8ISR6InZNEdzsAAADAfwhJHrC5ThIAAADgW4QkD9jx7nYeFwIAAABgN4QkDzickwQAAAD4FiHJA3S3AwAAAPyLkOQButsBAAAA/kVI8gDd7QAAAAD/IiR5IBaS6G4HAAAA+A8hyQOWxUgSAAAA4FeEJA848eskeVwIAAAAgN0QkjxAdzsAAADAvwhJHqC7HQAAAOBfhCQPxBs3MJIEAAAA+A4hyQO2zUgSAAAA4FeEJA848e52HhcCAAAAYDeEJA/Ezkliuh0AAADgP4QkD9j1W53pdgAAAID/EJI8sOs6SYQkAAAAwG8ISR6gux0AAADgX4QkD9DdDgAAAPAvQpIH4tPt6G4HAAAA+A4hyQOx6XYRptsBAAAAvkNI8kD9QBLT7QAAAAAfIiR5gMYNAAAAgH8RkjxAC3AAAADAvwhJHrA5JwkAAADwLUKSB3aNJHlcCAAAAIDdEJI8QHc7AAAAwL8ISR7gYrIAAACAfxGSPFCfkehuBwAAAPgQIckDsXOSGEkCAAAA/IeQ5IHYdDtjJENQAgAAAHyFkOSB2EiSRIc7AAAAwG8ISR6IjSRJdLgDAAAA/IaQ5AHHbjiSREgCAAAA/ISQ5IGG0+0YSQIAAAD8hZDkgQYZiQ53AAAAgM8QkjzQaLodI0kAAACArxCSPEB3OwAAAMC/CEkeoLsdAAAA4F+EJI/EptzR3Q4AAADwF0KSR2JT7hhJAgAAAPyFkOQRu37LE5IAAAAAfyEkecS2mG4HAAAA+BEhySNMtwMAAAD8iZDkETveuMHjQgAAAAA0QkjyCN3tAAAAAH8iJHnEZrodAAAA4EuEJI84dLcDAAAAfImQ5BGH7nYAAACALxGSPGIx3Q4AAADwJUKSR2jcAAAAAPgTIckjDi3AAQAAAF8iJHmkPiMx3Q4AAADwGUKSR+IjSYQkAAAAwFcISR6JXyeJc5IAAAAAX/F1SJo+fbosy2p069Gjh9dlNYvYSBLT7QAAAAB/CXhdwP706tVL8+fPj98PBHxf8gGhux0AAADgT75PHIFAQAUFBV6X0ex2XSfJ40IAAAAANOLr6XaS9OWXX6qoqEhdunTRuHHjtG7dun2uX1tbq/Ly8kY3P3LobgcAAAD4kq9D0oABAzR37ly9+uqrmj17ttasWaNTTjlFFRUVe33OzJkzlZ2dHb8VFxcnsOIDF5tuZ5huBwAAAPiKr0PS8OHD9aMf/Uh9+vTR0KFD9fLLL6u0tFRPPvnkXp8zdepUlZWVxW/r169PYMUHju52AAAAgD/5/pykhnJyctStWzetWrVqr+sEg0EFg8EEVtU0dLcDAAAA/MnXI0nfVllZqdWrV6uwsNDrUg4a3e0AAAAAf/J1SLr22mu1cOFCff3111q0aJHOO+88OY6jsWPHel3aQbPpbgcAAAD4kq+n223YsEFjx47Vtm3b1L59e5188sl677331L59e69LO2j1A0lymW4HAAAA+IqvQ9ITTzzhdQktJn5OEtPtAAAAAF/x9XS71iw23Y5zkgAAAAB/ISR5JN64gel2AAAAgK8Qkjxi0wIcAAAA8CVCkkec+MVkPS4EAAAAQCOEJI8w3Q4AAADwJ0KSR+oHkuhuBwAAAPgMIckj8el2jCQBAAAAvkJI8khsup1hJAkAAADwFUKSR3Z1t/O4EAAAAACNEJI8squ7HSNJAAAAgJ8QkjxCdzsAAADAnwhJHrEZSQIAAAB8iZDkkfqBJEaSAAAAAJ8hJHnEsWkBDgAAAPgRIckjse52ZCQAAADAXwhJiRIJS/95TXr7DikSjne3czknCQAAAPCVgNcFHDYsW3rqEilUJfX4YYPrJBGSAAAAAD9hJClRbFvK6xn9evMKrpMEAAAA+BQhKZHyj4n+u+UzOfVbnu52AAAAgL8QkhIpr1f0382fybKYbgcAAAD4ESEpkWIjSZtX7GoBznQ7AAAAwFcISYkUG0kqXauUSLUkiYwEAAAA+AshKZHS20oZBZKk3J1fSWK6HQAAAOA3hKREq59y17byS0lMtwMAAAD8hpCUaPnRKXdtq6Ihie52AAAAgL8QkhKt/rykNpWrJDHdDgAAAPAbQlKi1U+3y6n4UpKRy3Q7AAAAwFcISYnWrrtkOQqGypSvHYwkAQAAAD5DSEq0pBSp7dGSpB72en25pVKG0SQAAADANwhJXqifcneMs0EbduzUqi2VHhcEAAAAIIaQ5IX65g0nZ5ZIkl7/YouX1QAAAABogJDkhfo24D3t9ZIISQAAAICfEJK8UD/drk311woorA/X7lDZzpDHRQEAAACQCEneyO4oJWfIcut0attyRVyjt7/c6nVVAAAAAERI8oZtS3k9JUnnFuyQJL3+OVPuAAAAAD8gJHml/rykE1I3SZLe/M9WrpkEAAAA+AAhySv5x0qSiko/UGaKo+1VdfpkQ6m3NQEAAAAgJHmm+9mSE5S94QP94ohol7s36HIHAAAAeI6Q5JXsI6QTfiZJGlf1sCRDK3AAAADABwhJXjr5KikpTW1Ll+tMZ5k+3ViuDTuqva4KAAAAOKwRkryUmS+dOFGSdEPqM7Lk6sbnVsgYGjgAAAAAXiEkeW3Qr6XkTHUJr9Y5gaV6Y+VWPbpknddVAQAAAIctQpLX0nKlgVdIkm7Jfl62XP3hpc/11dZKjwsDAAAADk+EJD/4/hVSSo7aVH2lB3L/rppQSFf9Y5lCEdfrygAAAIDDDiHJD1JzpHPvkixHZ1S/pttT5mj5hh26/bWVnJ8EAAAAJBghyS+OGSmN/qtk2TpfC3RLYI7++tZqTX/hU7kuQQkAAABIlIDXBaCB3udLbkR69pf6SWCBJGna4ku0rapOf76gr4IBx+MCAQAAgNaPkSS/6XuhNOo+SZZ+Eligu5Pv0b+Wr9Olcz9URU3I6+oAAACAVo+Q5Eff+7F0/oOSnaRz7Pf0UPB2fbxqvcbMXqT127nYLACfq62Udqz1ugoAAJqMkORXx46Wxj0lJaVroLVCT6f+UeWb12rUve9q6drtXlcHAHv31Hjp7uOlTZ94XQkAAE1CSPKzo86QJrwopbVVT7Na/0q9XsfvXKSxf12iR5esVYSGDgD8pnyjtGq+5Ial5U96XQ0AAE1CSPK7I46Xfj5fKvyeskyF/pZ8h663HtSMZz/SD+9+R29/udXrCgFgly9e2vX15/+UuIwBAOAQREg6FOR2kS6dJw28UpI0IfAvLUy5Rj/YMlfXPvCqfvrAEr32aYlqQpGWq4EDHQAH4osXd31dulYq+bd3tQAA0ESWaeVXKy0vL1d2drbKysqUlZXldTkH78t50vOTpMrNkqSwsfWm21dL3e5aGzhS+d36q3/vY/S9jm1UmJ0iy7IO7vvVVUlPXSJt/UIa97TUvlszvAkArdLOHdLtR0en2hX0kUqWS6f+Vjrzd15XBgCApAPPBoSkQ1GoJjqN5cMHpXWLdnt4k8nV25He+jjYT9VHnKKjOxWrb3GO+nbIUXZa0nf7Po9fJH31RvR+m87SzxdI6W2b6Y0ALcgY6WA/JMB388kT0rO/lPKOkU6+SnrmF1L7ntKk97yuDAAASYSkuFYZkhra8oW08mWZkhWq+b/lSi79So4aT7vbaHL1lVuoNaZQ21I6qi67i6z2XZVT2EWd2mXpyHbp6pibppSkBherjYSkJy+WVr4sJaVLqW2k8g1Sp0HST5+TAsmJfZ/Ad7FqvvTs5VK/8dKZN3hdzeHjiXHR6Xan/lYaOKl+VCkkXfmh1K6r19UBAEBIimn1IenbQjultYsU/s88hf4zX6mlX+511TrjaKtytNVk6xuTrYpArnYmt1VNsJ2+567Q8ZVvKWwHNe+4e+Rk5umMd8YpKVypjUeO0eYz/qSiNmlqnxGUbfNpPXykbIN0/8nRqV+SNPJe6bifeFvT4aCuWrqtixTeKf3yLamwr/TImGhgHXyTdMrVXlcI+Evs8IsRbyChWlVIuvfee3X77berpKREffv21d13360TTzzxgJ572IWkb6veLm1bJW1bpZ2bvtDOkpWyt3+l9Mq1SjK1+3xqyDiaGLpab7jHSZJOsz/Rg0m3ybGMvnCLVa401SiocCBNYSdVYSdVESdNgYCjJMdWkiMFLVdpboXSIpVKiVTKdZJUm9xWdcE2qknKUVkkqB11jrbXOaoL5iirfUe1P6KzOhQWKTM1ScGAo2DAVjDJVjDgyGmBQLa5vEafrC9VTlqy+nTIbjyihkNLJCTN/aG0/j0pJVuqKZOcZOmSV6QOJ3hdXev2+YvSP8ZJ2R2lKcujB35L50r//LVUdLw08Q2vKwT8Y8fX0hM/kcI10oWPSHk9vK4IOGy0mpD0j3/8QxdffLHuv/9+DRgwQLNmzdJTTz2llStXKi8vb7/PP+xD0t64rlSxUarcLFOxWVXbN6pq+yZFyjdLlZvl1lbp/fwf6ePk/tpeXaeKmrDCEVeDy5/TpRWzW7y8WhNQSAFFZMdvrmxF5MiVLdeyZeTItaL3jWXLWNH7ISWpVkmqU7LqrGS5TrJcOyjXCcoEgjJOUAqkqDZsVLpjm1Rbrgxrp+pMQBV2ptKy2im3bTsFkpJlOwFZTvRfOxCQ4yTJdpIkJyDLdmTZjmQHZNsBybFl2Umynegyy46uE32N6H3bCch2nOjXgejzLCcgxwnItm3ZtiXHsmTbkm3Vf21Zsi0jJ1Qhp/ob2bXlsm1LtiVZsqS0XCm7g+Ts43wz15UitVIgpXV/ajn/ZumdO6RgljTxTWnetOj0r8zC6P3MAq8rbL2e+aW0/Anp+1dIw2ZGl1Vulf7UVZKRpqyQcoo9LRHwhY3LpEd/JFVtid5PyVb4gke1te0Jys9MYXYG0MJaTUgaMGCA+vfvr3vuuUeS5LquiouLNXnyZP3Xf/3Xfp9PSGoBW/8jla5VpKZS5RVlqq4sV7imSpHaSrm11QpHwgpFpFDEVZ2xVWVlqNrJUKWVLitcp9TQDqWGtisjUqYMO6QMu06pVp2SancoZWeJMiJlXr9DT0SMVR8Ind3CYaaqFbTCe31u2NjarFxtUa4kS47lKqCI0lWjLFUqS5Vy5KpaKfrGytU2u63qrKCCplYpqlWy6hRRQHVWUGErScayFVBYySYkRxHttNNV5WSpyslWrZ0mSdGQZlmyLMlS9GtZllwrWREnWa6dpCQTVrKpUdCtkaOwQk5a/chjmjJD36hd9Wq1rV6t9Lqt2pF6pErSu2tTandVJreTY9uyLFuOY8m2bTm2Lce2ZNcvcyxbtmPLGKOwK6VWb9IJn/5ekvRQh+laYA+Sair0+21T1DGyTmucI7Uy6yRZWUVKze2gQGqGLCdJspMk25GjiBwZ2XLrfwKuHOPKkpEcR8aKhl9jByQrsGuZogc0lowC5euV8s0KpW77VKmVa1WRVqxtmT20Oa27yoOFkmXJsmxZlpRp1yrT2qkMVSvoSJGUXIVTcuUGc2TbliwTkWPCioTqVFlTq+qdNaqurZXlJCspJV1JwXQFU1KU6hilOa5SHVeWLIVlKWwsuZYtx0lSIOAoEEiSqrfL3bFOKlsvVW5WyDiqU7JqrSRFkjJlZ+YpKatASdl5su2keJa24z9jS5Zl6t9r9Odtx372ctX2odNk15bqkx88rtWpfbSxdKfWfFOtn315hXqFVuixlIv0TedzdUzHfHU7op2SAk59YLdkLEuSHb+/6181eLx+O7thWZHa6KihjGQnydjRDy4UCcmK1MoK18iqq5JdWyq7plRWbblqA1mqTG6v8qT2qkvKUkbAKD3JVUZAco1RnWupNiIZS0pNDiotGFBaMCjLcWTkKCJLEdfIuGFFQrUy4ZBc4ypiYh/aWJLtyLYDsm0n+kGGHfuQY9f2si1Ljh3dprZlyZIUdo1CEVehiJFrjJJsWwEnul7Zju0q3/K1qr9Zp3DtTpnMfDmZhQpkFyo1JVmpyQGlBWylBCzZllv/F8NEX9t2JMtucDuAg2/XVaRqm2pLNypU8Y122unamdRGVYE2UiCorJQkZaUGlJmStNvIvjFGNSFX1XVhBezYDAD7O3dZDUdc1YRdhcKukgPR1wg4dn15RnURN769QhFXdWFXoVCdwnW1itTtVCRcq/S0NGVn5Sg7I11O/XNlTLTzoh1o2odFpeuldYultYtkStcrlNtVO9v3UXmbXlJ2R2WkpSkjNUlJzt6vrmJWLZD5x09lh6q0Lb2ryt1kdd75qepMQFNCV+jjzNN1bt8ijehbpF5FWQffofYQZ1xXO6srVFezUykZ2QoGm6Frb/zF/T3dsbSqRmu+XqOS9avkhkNq1+EodT7yaLXPTjvs94uD1SpCUl1dndLS0vT0009r1KhR8eXjx49XaWmpnn/++d2eU1tbq9raXdPIysvLVVxcTEg6lIRrpcot0RO+XVfhcJ1C4bBCdSGFImHV1dUpEg4rHAkrEg4rEgkpHI4oEg7LjYTkuCE5bq0CplZWuE5uaKcidTUyoRqZcE10ekO4VrZcZee0Vbt27ZWcli0TrlX5jq3a/k2JaipLJTccPSAzYVluRJYJy3bDsk1YlokejFgmGmksU39gbdx4tHFMpPEBd/3tYFSYVJUrLXpAVq+9yhS0Qge50VuPv4eH6Mbwz+L3O1kleiH5BmVb1R5WdXj4xmTpxNr75Da4BN8lziu6KenvHlbVfFwTHdU90HUlKbZ27Hd217+Nl2sPy22Zvf5uu/UfqjhyD7gmKfphTP1fLxnt/nWaapRk7fmae3XGafS3Z1fVsZr3pCkHc7EwviuUS5IrW7UKqE5JishWksIKKqSgQgpYe/7bGja2wnIUUCS+jmss1ShZtUpWWLYCiihJYSXVNz1yZdXf7Pi/ttz9/g0JG1vVCqpWyTINfgeMteujlFxTqoDl6p1IL10eukp1CuiupHs01PlQkrTVxI5TrPpj9137zb73nV1r2PX/z1j1y2yZ+NrRd7TrFe36f2Pr77pvZOr/NwvLUbj+Y6RI/deWjJIUVrJCSlJYYTmqVbJCCiisQPw1dlW4+zuI3bd2+3lH1002IaWpptH+XW2CqrTSVKfv0Km3gYBcpai2/lYnKbo/GKn+44XY74KlkBWdlRLdTwLRDwUb1Gik+qBl6t9S4/3Vkol/iCip/udgaU+/Kd/eApZctTU7lPyt38WQcbTFypUrR0kKKVkhWTIKK6CQFd2LA4ooWbUKmjo5iqi2fl+vUVARy45/e/Ot7x/98KvBh57fKvC7hAVrL3e2ZH9PA37t/f8HBxqSAgms6Tv75ptvFIlElJ+f32h5fn6+vvjiiz0+Z+bMmbr55psTUR5aSiDYaFpOoP6W2sLf1pKUXX9rMcZIxo1+mulGov+aSP3XDe+HZdyIIuFQ9N/kLLmpuZKTogw3+sl3xBi5rtEO15Wp3Cy7dK3sqs3RSGZFx0RCTppCyTkKB3MUclLkVH+jQGWJnKpNUqROESdVITtFETs5GkrDtbLCO2VcV2E7SRErWRE5cuoqlFS3Q8m1O+SEq6MHVcZE346inyBH/49wZUfqZLu1siN1ilhJqrNTVGenKmI5CoSrlRypVlKkWlVOtkpSuqgkpbNK7XYqCn2tDrVfqmjnl0oJl0syMsZIpv5fmfr/j1zFP9sxbv0fdCNLljakdte6Ltfouuwstc1IVkYwoGDA1pc7v6fsNS8ptOP/pMoSJe/crIBbq4AJK2BCsowr19o1nTMcn95pyzV2POzGbyZSH3p3/QdmJJVZ2foqcLS+Tj5a3yR3UAdToi7hVTqybpUyIjvi/wXKSDVWiiqVqgqlyTVGWaZC2aZcWaZCRlb8YCSsQHQqqR2QLEeOwkpya5Rs6hQwofgBTMhEz6Vz6kcTnPpDHKc+0FcoTZusPG222muH01bJtlGqFVKqVac0t1JZ4R3KdncoW+Vy9vHfYezg/9vCsvWYPUJd87PVPjOovKygurRLV7fMzqr+4FPZZeul8E7Zkeih1MGKvd9vH9BHjKVaJWungipVhsqUoUqTqly7UvnaoVyVNvqwIvZ+DiRofJcwsvu6Tf88slzp2ma3U9hOUba7XbnudgWs6Fjzd+VYZrcOqHuyzWRqh8lUlrVTbaxyJSmy2wFb4kUUVEjSzv2sZcd/xgHLVeBbH07ZllGaapWmfZ+X+21hY2uFOVIfuD30tSlQN2u9ettf6xjra6VY0aCWpZ37rs+SnosM0v05V+vUwlz1yM+Ulf+wKv7zJ2Uuf1DtrfLvVFPLcr9TFMlUM38QtYc/NWnWd/+57U/sd/Xbvxep9SEqbl+/wi0xsGNF9+VSp61cK6A24S1KsiI6Qlv3vP5e6ktTzX7X2e3x5hw+afBaFTv3f5qMn/h6JGnjxo064ogjtGjRIg0cODC+/Le//a0WLlyoJUuW7PYcRpIA4BBQH4CjH1G6uz5AUMPlDf7Hjn3tJEdvdoMpVJFQ9AMGJzk67W5fImEpVF2/bnSqZaN6TEQyriKRsKpramWMiY8K25YrJxCUHUiWHUiWZNXXXv8hh3HrPwSJKBrq3egML9eVayS3PuC7xsg1knGjXwdsKeBYSq6fhheOuIq4Uth1lZadJzslo/F7cF2pelv91DFHIVeqjSg67c9ExxGinye4ct2IjOtKitZmxbZzvFZXxrhyI2G5kYjslHQlZxcoNSVVwUD0PEkZI9WURi8uruhU6qraiOoirsIRV2E3OiKQmuwoLTmglICtsGtUG46oNhxROGLiH2zsFh3jnxsYGVfR6XVJjlKSHSU5Tv10Oqk24spyw0qqH7kIKKxAUorspJToB2uB+n+dYHzqZaimUuVlZaqtq5Wxk2Q50emythuWFa6WHa6JzhhwkmSc5OjUWys61dWKbSu58X3CzSqWkjPiUyXTkqONhSzjSnWVCtdUamdlhap3Vsq4pv6DpF0/bxkjk5ym/M7H7rlBUOl6qbZCklFVbVhbKmq0K2t/6+g19jsS+zo+RdVWbKqqaXTfkol9LdUPGUTvm9gUVys2+hWd4mrV/z5YbkiWG5FM/ewKNzr12zjJcp1gdLvVT4O1I7XR/bLBdNmG3zP+vRrW0WgdxddJSk5VSka20jKyFExJU3VlqWoqtqumYofcUNOCkrEcmUCq3ECqTCBFAcdWwJKSnOhIT6T+d8aNuHLDNTKhWpnwTplwnSImOt3TlWTLik4BdywFYtPBnegySYoYKeKa+HPCrhQxsfGa2PuOb+3YW5dM/eiTZSmvsKPS23bY9TfNjahmxwZtXr9aYdfIrv9bZCxbkVBt9FZXIzlJcp1UmaQUWXZATqRWjlsjJ1JTP0YWmwLceNTIGBOdwuq6Ckdio367ptTvofT65+1rgze+G8xqpyN7DfgOP7GW0SpGktq1ayfHcbR58+ZGyzdv3qyCgj2fgB0MBhUMBhNRHgCgqSyrwf+2B9FR0rLqr9t2gNducwKSs4f/FOP12PWrBZUZTG96Xdp16Lf3M1T2LKn+tle2LWW0P/D1D5ZlRa+Vl9om/v1y9vOU2E8k8yC/tSMppUlPTFJSehu1TW9zkBUcAMuRUrIVSMlWZs5BvOcGMyjSJXVuhtJak8yc9srMab//FVsr21FK207q1LaT15UcNr7r3+6ESk5OVr9+/bRgwYL4Mtd1tWDBgkYjSwAAAADQXHw9kiRJV199tcaPH68TTjhBJ554ombNmqWqqipdcsklXpcGAAAAoBXyfUi68MILtXXrVk2bNk0lJSX63ve+p1dffXW3Zg4AAAAA0Bx83bihOXCdJAAAAADSgWcDX5+TBAAAAACJRkgCAAAAgAYISQAAAADQACEJAAAAABogJAEAAABAA4QkAAAAAGiAkAQAAAAADRCSAAAAAKABQhIAAAAANEBIAgAAAIAGAl4X0NKMMZKk8vJyjysBAAAA4KVYJohlhL1p9SGpoqJCklRcXOxxJQAAAAD8oKKiQtnZ2Xt93DL7i1GHONd1tXHjRmVmZsqyLE9rKS8vV3FxsdavX6+srCxPa2mt2MYti+3b8tjGLYvt2/LYxi2L7dvy2MYty+vta4xRRUWFioqKZNt7P/Oo1Y8k2batDh06eF1GI1lZWfzStTC2ccti+7Y8tnHLYvu2PLZxy2L7tjy2ccvycvvuawQphsYNAAAAANAAIQkAAAAAGiAkJVAwGNRNN92kYDDodSmtFtu4ZbF9Wx7buGWxfVse27hlsX1bHtu4ZR0q27fVN24AAAAAgO+CkSQAAAAAaICQBAAAAAANEJIAAAAAoAFCEgAAAAA0QEhKoHvvvVdHHnmkUlJSNGDAAL3//vtel3RImjlzpvr376/MzEzl5eVp1KhRWrlyZaN1Tj/9dFmW1eh22WWXeVTxoWX69Om7bbsePXrEH6+pqdGkSZPUtm1bZWRkaMyYMdq8ebOHFR96jjzyyN22sWVZmjRpkiT236Z46623NGLECBUVFcmyLD333HONHjfGaNq0aSosLFRqaqqGDBmiL7/8stE627dv17hx45SVlaWcnBxdeumlqqysTOC78K99bd9QKKTrrrtOvXv3Vnp6uoqKinTxxRdr48aNjV5jT/v9rbfemuB34l/724cnTJiw2/YbNmxYo3XYh/duf9t3T3+TLcvS7bffHl+HfXjvDuTY7ECOH9atW6dzzjlHaWlpysvL029+8xuFw+FEvpU4QlKC/OMf/9DVV1+tm266SR999JH69u2roUOHasuWLV6XdshZuHChJk2apPfee0/z5s1TKBTSWWedpaqqqkbr/eIXv9CmTZvit9tuu82jig89vXr1arTt3nnnnfhjV111lf75z3/qqaee0sKFC7Vx40aNHj3aw2oPPR988EGj7Ttv3jxJ0o9+9KP4Ouy/301VVZX69u2re++9d4+P33bbbbrrrrt0//33a8mSJUpPT9fQoUNVU1MTX2fcuHH69NNPNW/ePL344ot66623NHHixES9BV/b1/atrq7WRx99pBtvvFEfffSRnnnmGa1cuVLnnnvubuvOmDGj0X49efLkRJR/SNjfPixJw4YNa7T9Hn/88UaPsw/v3f62b8PtumnTJj344IOyLEtjxoxptB778J4dyLHZ/o4fIpGIzjnnHNXV1WnRokV66KGHNHfuXE2bNs2LtyQZJMSJJ55oJk2aFL8fiURMUVGRmTlzpodVtQ5btmwxkszChQvjy0477TTz61//2ruiDmE33XST6du37x4fKy0tNUlJSeapp56KL/v888+NJLN48eIEVdj6/PrXvzZHHXWUcV3XGMP+e7AkmWeffTZ+33VdU1BQYG6//fb4stLSUhMMBs3jjz9ujDHms88+M5LMBx98EF/nlVdeMZZlmf/7v/9LWO2Hgm9v3z15//33jSSzdu3a+LJOnTqZO++8s2WLayX2tI3Hjx9vRo4cudfnsA8fuAPZh0eOHGnOPPPMRsvYhw/ct4/NDuT44eWXXza2bZuSkpL4OrNnzzZZWVmmtrY2sW/AGMNIUgLU1dVp6dKlGjJkSHyZbdsaMmSIFi9e7GFlrUNZWZkkKTc3t9HyRx99VO3atdOxxx6rqVOnqrq62ovyDklffvmlioqK1KVLF40bN07r1q2TJC1dulShUKjRvtyjRw917NiRfbmJ6urq9Mgjj+hnP/uZLMuKL2f/bT5r1qxRSUlJo/02OztbAwYMiO+3ixcvVk5Ojk444YT4OkOGDJFt21qyZEnCaz7UlZWVybIs5eTkNFp+6623qm3btjruuON0++23ezaN5lD15ptvKi8vT927d9fll1+ubdu2xR9jH24+mzdv1ksvvaRLL710t8fYhw/Mt4/NDuT4YfHixerdu7fy8/Pj6wwdOlTl5eX69NNPE1h9VCDh3/Ew9M033ygSiTT6oUtSfn6+vvjiC4+qah1c19WUKVM0aNAgHXvssfHlP/7xj9WpUycVFRVp+fLluu6667Ry5Uo988wzHlZ7aBgwYIDmzp2r7t27a9OmTbr55pt1yimnaMWKFSopKVFycvJuBz75+fkqKSnxpuBD3HPPPafS0lJNmDAhvoz9t3nF9s09/Q2OPVZSUqK8vLxGjwcCAeXm5rJvf0c1NTW67rrrNHbsWGVlZcWX/+pXv9Lxxx+v3NxcLVq0SFOnTtWmTZt0xx13eFjtoWPYsGEaPXq0OnfurNWrV+v666/X8OHDtXjxYjmOwz7cjB566CFlZmbuNpWcffjA7OnY7ECOH0pKSvb4dzr2WKIRknBImzRpklasWNHonBlJjeZg9+7dW4WFhRo8eLBWr16to446KtFlHlKGDx8e/7pPnz4aMGCAOnXqpCeffFKpqakeVtY6PfDAAxo+fLiKioriy9h/cagKhUK64IILZIzR7NmzGz129dVXx7/u06ePkpOT9ctf/lIzZ85UMBhMdKmHnIsuuij+de/evdWnTx8dddRRevPNNzV48GAPK2t9HnzwQY0bN04pKSmNlrMPH5i9HZsdaphulwDt2rWT4zi7dfDYvHmzCgoKPKrq0HfllVfqxRdf1BtvvKEOHTrsc90BAwZIklatWpWI0lqVnJwcdevWTatWrVJBQYHq6upUWlraaB325aZZu3at5s+fr5///Of7XI/99+DE9s19/Q0uKCjYrZFOOBzW9u3b2bcPUCwgrV27VvPmzWs0irQnAwYMUDgc1tdff52YAluZLl26qF27dvG/C+zDzePtt9/WypUr9/t3WWIf3pO9HZsdyPFDQUHBHv9Oxx5LNEJSAiQnJ6tfv35asGBBfJnrulqwYIEGDhzoYWWHJmOMrrzySj377LN6/fXX1blz5/0+Z9myZZKkwsLCFq6u9amsrNTq1atVWFiofv36KSkpqdG+vHLlSq1bt459uQnmzJmjvLw8nXPOOftcj/334HTu3FkFBQWN9tvy8nItWbIkvt8OHDhQpaWlWrp0aXyd119/Xa7rxkMq9i4WkL788kvNnz9fbdu23e9zli1bJtu2d5sihgOzYcMGbdu2Lf53gX24eTzwwAPq16+f+vbtu9912Yd32d+x2YEcPwwcOFD//ve/G4X92AcuxxxzTGLeSEMJbxVxmHriiSdMMBg0c+fONZ999pmZOHGiycnJadTBAwfm8ssvN9nZ2ebNN980mzZtit+qq6uNMcasWrXKzJgxw3z44YdmzZo15vnnnzddunQxp556qseVHxquueYa8+abb5o1a9aYd9991wwZMsS0a9fObNmyxRhjzGWXXWY6duxoXn/9dfPhhx+agQMHmoEDB3pc9aEnEomYjh07muuuu67RcvbfpqmoqDAff/yx+fjjj40kc8cdd5iPP/443l3t1ltvNTk5Oeb55583y5cvNyNHjjSdO3c2O3fujL/GsGHDzHHHHWeWLFli3nnnHdO1a1czduxYr96Sr+xr+9bV1Zlzzz3XdOjQwSxbtqzR3+VYR6pFixaZO++80yxbtsysXr3aPPLII6Z9+/bm4osv9vid+ce+tnFFRYW59tprzeLFi82aNWvM/PnzzfHHH2+6du1qampq4q/BPrx3+/sbYYwxZWVlJi0tzcyePXu357MP79v+js2M2f/xQzgcNscee6w566yzzLJly8yrr75q2rdvb6ZOnerFWzKEpAS6++67TceOHU1ycrI58cQTzXvvved1SYckSXu8zZkzxxhjzLp168ypp55qcnNzTTAYNEcffbT5zW9+Y8rKyrwt/BBx4YUXmsLCQpOcnGyOOOIIc+GFF5pVq1bFH9+5c6e54oorTJs2bUxaWpo577zzzKZNmzys+ND02muvGUlm5cqVjZaz/zbNG2+8sce/C+PHjzfGRNuA33jjjSY/P98Eg0EzePDg3bb9tm3bzNixY01GRobJysoyl1xyiamoqPDg3fjPvrbvmjVr9vp3+Y033jDGGLN06VIzYMAAk52dbVJSUkzPnj3NH//4x0YH+Ie7fW3j6upqc9ZZZ5n27dubpKQk06lTJ/OLX/xitw9a2Yf3bn9/I4wx5i9/+YtJTU01paWluz2ffXjf9ndsZsyBHT98/fXXZvjw4SY1NdW0a9fOXHPNNSYUCiX43URZxhjTQoNUAAAAAHDI4ZwkAAAAAGiAkAQAAAAADRCSAAAAAKABQhIAAAAANEBIAgAAAIAGCEkAAAAA0AAhCQAAAAAaICQBANCAZVl67rnnvC4DAOAhQhIAwDcmTJggy7J2uw0bNszr0gAAh5GA1wUAANDQsGHDNGfOnEbLgsGgR9UAAA5HjCQBAHwlGAyqoKCg0a1NmzaSolPhZs+ereHDhys1NVVdunTR008/3ej5//73v3XmmWcqNTVVbdu21cSJE1VZWdlonQcffFC9evVSMBhUYWGhrrzyykaPf/PNNzrvvPOUlpamrl276oUXXog/tmPHDo0bN07t27dXamqqunbtuluoAwAc2ghJAIBDyo033qgxY8bok08+0bhx43TRRRfp888/lyRVVVVp6NChatOmjT744AM99dRTmj9/fqMQNHv2bE2aNEkTJ07Uv//9b73wwgs6+uijG32Pm2++WRdccIGWL1+us88+W+PGjdP27dvj3/+zzz7TK6+8os8//1yzZ89Wu3btErcBAAAtzjLGGK+LAABAip6T9MgjjyglJaXR8uuvv17XX3+9LMvSZZddptmzZ8cf+/73v6/jjz9e9913n/72t7/puuuu0/r165Weni5JevnllzVixAht3LhR+fn5OuKII3TJJZfo97///R5rsCxLN9xwg2655RZJ0eCVkZGhV155RcOGDdO5556rdu3a6cEHH2yhrQAA8BrnJAEAfOWMM85oFIIkKTc3N/71wIEDGz02cOBALVu2TJL0+eefq2/fvvGAJEmDBg2S67pauXKlLMvSxo0bNXjw4H3W0KdPn/jX6enpysrK0pYtWyRJl19+ucaMGaOPPvpIZ511lkaNGqWTTjqpSe8VAOBPhCQAgK+kp6fvNv2tuaSmph7QeklJSY3uW5Yl13UlScOHD9fatWv18ssva968eRo8eLAmTZqkP/3pT81eLwDAG5yTBAA4pLz33nu73e/Zs6ckqWfPnvrkk09UVVUVf/zdd9+Vbdvq3r27MjMzdeSRR2rBggUHVUP79u01fvx4PfLII5o1a5b++te/HtTrAQD8hZEkAICv1NbWqqSkpNGyQCAQb47w1FNP6YQTTtDJJ5+sRx99VO+//74eeOABSdK4ceN00003afz48Zo+fbq2bt2qyZMn66c//any8/MlSdOnT9dll12mvLw8DR8+XBUVFXr33Xc1efLkA6pv2rRp6tevn3r16qXa2lq9+OKL8ZAGAGgdCEkAAF959dVXVVhY2GhZ9+7d9cUXX0iKdp574okndMUVV6iwsFCPP/64jjnmGElSWlqaXnvtNf36179W//79lZaWpjFjxuiOO+6Iv9b48eNVU1OjO++8U9dee63atWun888//4DrS05O1tSpU/X1118rNTVVp5xyip544olmeOcAAL+gux0A4JBhWZaeffZZjRo1yutSAACtGOckAQAAAEADhCQAAAAAaIBzkgAAhwxmiAMAEoGRJAAAAABogJAEAAAAAA0QkgAAAACgAUISAAAAADRASAIAAACABghJAAAAANAAIQkAAAAAGiAkAQAAAEADhCQAAAAAaOD/AwQ62ZDDA2dmAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training time: 34043.1511 seconds\n"
          ]
        }
      ],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "#plt.savefig(\"/content/drive/My Drive/Project 2 Neuroimage Synthesis/loss_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "# Total training time\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f'Total training time: {total_time:.4f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reload Model"
      ],
      "metadata": {
        "id": "FtIlgsWeQIFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model\n",
        "model = UNet(2, 2).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "ObLyW-Yjxr4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "#model_path = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/unet.pt\"\n",
        "\n",
        "model_path = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/attention.pt\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Switch to eval mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1S4yaUyRes",
        "outputId": "d7455570-ba6c-4c7a-fe04-117cc61f3436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (enc1): Conv3d(2, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (enc2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (enc3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (enc4): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (middle): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (attention): LocalAttention(\n",
              "    (conv_W): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (softmax): Softmax(dim=-1)\n",
              "  )\n",
              "  (up4): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec4): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (up3): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec3): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (up2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec2): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (up1): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec1): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (output): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg43aZtYuEHE"
      },
      "source": [
        "## Test - Produce Synthesized Maps of FA and ADC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6C0trMP6uNF",
        "outputId": "10d94b91-747e-4502-9ddd-72ac36b59517"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['103',\n",
              " '177',\n",
              " '140',\n",
              " '184',\n",
              " '042',\n",
              " '111',\n",
              " '023',\n",
              " '020',\n",
              " '165',\n",
              " '065',\n",
              " '048',\n",
              " '114',\n",
              " '141',\n",
              " '183',\n",
              " '015',\n",
              " '142',\n",
              " '196',\n",
              " '037',\n",
              " '098',\n",
              " '195',\n",
              " '137']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "test_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import nibabel as nib\n",
        "from nibabel.processing import resample_to_output\n",
        "import csv\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import mean_squared_error as mse"
      ],
      "metadata": {
        "id": "FNxWCkFdytni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nilearn.image import resample_img\n",
        "\n",
        "# Define the new voxel size for resampling\n",
        "new_voxel_size = (1.25, 1.25, 1.25)\n",
        "\n",
        "successful_patients = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (input_imgs, target_imgs) in enumerate(test_loader):\n",
        "        patient_id = test_ids[i]\n",
        "        print(f\"Processing patient {patient_id}\")\n",
        "\n",
        "        # Save the original images for visualization\n",
        "        original_input_imgs = input_imgs.clone()\n",
        "\n",
        "        # Move tensors to the device\n",
        "        input_imgs = input_imgs.to(device)\n",
        "        target_imgs = target_imgs.to(device)\n",
        "\n",
        "        # Generate output maps from input images\n",
        "        outputs = model(input_imgs)\n",
        "\n",
        "        # Check the number of outputs\n",
        "        if len(outputs) == 2:\n",
        "            # Unpack the outputs\n",
        "            fake_fa, fake_adc = outputs\n",
        "\n",
        "            # Convert tensors to Nifti images\n",
        "            fake_fa_nifti = nib.Nifti1Image(fake_fa[0].cpu().numpy(), np.eye(4))\n",
        "            fake_adc_nifti = nib.Nifti1Image(fake_adc[0].cpu().numpy(), np.eye(4))\n",
        "\n",
        "            # Resample the images to the new voxel size\n",
        "            fake_fa_nifti = resample_img(fake_fa_nifti, target_affine=np.diag(new_voxel_size))\n",
        "            fake_adc_nifti = resample_img(fake_adc_nifti, target_affine=np.diag(new_voxel_size))\n",
        "\n",
        "            # Save the synthesized FA and ADC maps\n",
        "            patient_id = test_ids[i]\n",
        "            patient_folder = os.path.join(root_dir, patient_id)\n",
        "\n",
        "            fa_syn_path = os.path.join(patient_folder, 'FA_syn.nii.gz')\n",
        "            adc_syn_path = os.path.join(patient_folder, 'ADC_syn.nii.gz')\n",
        "\n",
        "            nib.save(fake_fa_nifti, fa_syn_path)\n",
        "            nib.save(fake_adc_nifti, adc_syn_path)\n",
        "\n",
        "            # Add the successful patient_id to the list\n",
        "            successful_patients.append(patient_id)\n",
        "        elif len(outputs) == 1:\n",
        "            print(f\"Only one output for patient {test_ids[i]}, skipping.\")\n",
        "        else:\n",
        "            print(f\"Unexpected number of outputs for patient {test_ids[i]}, skipping.\")\n",
        "\n",
        "print(\"Successfully saved patients:\", successful_patients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM5pTFajyzP5",
        "outputId": "8fb753dc-39ff-470c-ad28-556051c783c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing patient 103\n",
            "Processing patient 177\n",
            "Processing patient 140\n",
            "Processing patient 184\n",
            "Processing patient 042\n",
            "Processing patient 111\n",
            "Processing patient 023\n",
            "Processing patient 020\n",
            "Processing patient 165\n",
            "Processing patient 065\n",
            "Processing patient 048\n",
            "Only one output for patient 048, skipping.\n",
            "Successfully saved patients: ['103', '177', '140', '184', '042', '111', '023', '020', '165', '065']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzrTJHVf8i_i"
      },
      "source": [
        "## Save Metrics (MSE, PSNR, SSIM) to a CSV comparing synthesized FA and ADC maps to original FA and ADC for each patient in test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfGyPvbXopmC"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from skimage.metrics import mean_squared_error as mse, peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from nilearn.image import resample_to_img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FKz0gFSowKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19c03c7-da66-42b1-ca3d-91ced7769c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE FA: 0.03228868524982706\n",
            "Average PSNR FA: 9.369775048179037\n",
            "Average SSIM FA: 0.7024565985870111\n",
            "Average MSE ADC: 0.002941875731995592\n",
            "Average PSNR ADC: 21.60639002461108\n",
            "Average SSIM ADC: 0.957304768329792\n",
            "Average MAE FA: 0.07470827453586766\n",
            "Average MAE ADC: 0.010496758324551294\n",
            "Metrics saved to metrics.csv\n"
          ]
        }
      ],
      "source": [
        "# Define the headers for the CSV file\n",
        "headers = ['Patient_ID', 'MSE_FA', 'PSNR_FA', 'SSIM_FA', 'MAE_FA', 'MSE_ADC', 'PSNR_ADC', 'SSIM_ADC', 'MAE_ADC']\n",
        "\n",
        "\n",
        "# Initialize running sums for calculating the averages\n",
        "sum_mse_fa, sum_psnr_fa, sum_ssim_fa = 0, 0, 0\n",
        "sum_mse_adc, sum_psnr_adc, sum_ssim_adc = 0, 0, 0\n",
        "sum_mae_fa, sum_mae_adc = 0, 0\n",
        "\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open('attention_metrics.csv', 'w', newline='') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(headers)\n",
        "\n",
        "    for patient_id in successful_patients:\n",
        "        patient_folder = os.path.join(root_dir, patient_id)\n",
        "\n",
        "        fa_syn_path = os.path.join(patient_folder, 'FA_syn.nii.gz')\n",
        "        adc_syn_path = os.path.join(patient_folder, 'ADC_syn.nii.gz')\n",
        "        fa_deformed_path = os.path.join(patient_folder, 'FA_deformed.nii.gz')\n",
        "        adc_deformed_path = os.path.join(patient_folder, 'ADC_deformed.nii.gz')\n",
        "\n",
        "        fa_syn_nifti = nib.load(fa_syn_path)\n",
        "        fa_syn = fa_syn_nifti.get_fdata()\n",
        "        adc_syn_nifti = nib.load(adc_syn_path)\n",
        "        adc_syn = adc_syn_nifti.get_fdata()\n",
        "\n",
        "        fa_deformed_nifti = nib.load(fa_deformed_path)\n",
        "        fa_deformed = fa_deformed_nifti.get_fdata()\n",
        "        adc_deformed_nifti = nib.load(adc_deformed_path)\n",
        "        adc_deformed = adc_deformed_nifti.get_fdata()\n",
        "\n",
        "        # Resample the synthesized FA and ADC images to match the deformed FA and ADC images\n",
        "        fa_syn_nifti = resample_to_img(fa_syn_nifti, fa_deformed_nifti)\n",
        "        fa_syn = fa_syn_nifti.get_fdata()\n",
        "        adc_syn_nifti = resample_to_img(adc_syn_nifti, adc_deformed_nifti)\n",
        "        adc_syn = adc_syn_nifti.get_fdata()\n",
        "\n",
        "        mae_fa = mean_absolute_error(fa_syn.flatten(), fa_deformed.flatten())\n",
        "        mse_fa = mse(fa_syn, fa_deformed)\n",
        "        psnr_fa = psnr(fa_syn, fa_deformed, data_range=fa_syn.max() - fa_syn.min())\n",
        "        ssim_fa = ssim(fa_syn, fa_deformed)\n",
        "\n",
        "        mae_adc = mean_absolute_error(adc_syn.flatten(), adc_deformed.flatten())\n",
        "        mse_adc = mse(adc_syn, adc_deformed)\n",
        "        psnr_adc = psnr(adc_syn, adc_deformed, data_range=adc_syn.max() - adc_syn.min())\n",
        "        ssim_adc = ssim(adc_syn, adc_deformed)\n",
        "\n",
        "        # Write the metrics to the CSV file\n",
        "        csv_writer.writerow([patient_id, mse_fa, psnr_fa, ssim_fa, mae_fa, mse_adc, psnr_adc, ssim_adc, mae_adc])\n",
        "\n",
        "        # Update the running sums\n",
        "        sum_mse_fa += mse_fa\n",
        "        sum_psnr_fa += psnr_fa\n",
        "        sum_ssim_fa += ssim_fa\n",
        "        sum_mse_adc += mse_adc\n",
        "        sum_psnr_adc += psnr_adc\n",
        "        sum_ssim_adc += ssim_adc\n",
        "\n",
        "        sum_mae_fa += mae_fa\n",
        "        sum_mae_adc += mae_adc\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the averages\n",
        "num_successful_patients = len(successful_patients)\n",
        "avg_mse_fa = sum_mse_fa / num_successful_patients\n",
        "avg_psnr_fa = sum_psnr_fa / num_successful_patients\n",
        "avg_ssim_fa = sum_ssim_fa / num_successful_patients\n",
        "avg_mse_adc = sum_mse_adc / num_successful_patients\n",
        "avg_psnr_adc = sum_psnr_adc / num_successful_patients\n",
        "avg_ssim_adc = sum_ssim_adc / num_successful_patients\n",
        "avg_mae_fa = sum_mae_fa / num_successful_patients\n",
        "avg_mae_adc = sum_mae_adc / num_successful_patients\n",
        "\n",
        "\n",
        "# Output the averages\n",
        "print(\"Average MSE FA:\", avg_mse_fa)\n",
        "print(\"Average PSNR FA:\", avg_psnr_fa)\n",
        "print(\"Average SSIM FA:\", avg_ssim_fa)\n",
        "print(\"Average MSE ADC:\", avg_mse_adc)\n",
        "print(\"Average PSNR ADC:\", avg_psnr_adc)\n",
        "print(\"Average SSIM ADC:\", avg_ssim_adc)\n",
        "print(\"Average MAE FA:\", avg_mae_fa)\n",
        "print(\"Average MAE ADC:\", avg_mae_adc)\n",
        "\n",
        "\n",
        "print(\"Metrics saved to metrics.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}