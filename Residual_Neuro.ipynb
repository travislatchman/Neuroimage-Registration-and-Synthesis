{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/travislatchman/Neuroimage-Registration-and-Synthesis/blob/main/Residual_Neuro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "Izs81XtnVj_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW0EUVHtQOqt",
        "outputId": "5e64ac44-1fc4-4db4-938f-d6c1f3d0649b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.1-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.2)\n",
            "Collecting nibabel>=3.2.0 (from nilearn)\n",
            "  Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nibabel, nilearn\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "Successfully installed nibabel-5.1.0 nilearn-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.22.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel) (23.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn\n",
        "!pip install nibabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBYSQ831xUwZ",
        "outputId": "ea826a4d-4f33-479a-f9b8-62ca48a10a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=e25e9ff6fec8e111e9ae72a80e90a233393f06c6690db10cd40f409a3931e85e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpXCwqlIqWP5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import mean_squared_error as mse, peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
        "import csv\n",
        "from torchviz import make_dot\n",
        "import torch.onnx\n",
        "import random\n",
        "import time\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SsbKTHpQLMU",
        "outputId": "2f231569-caee-4d0f-ba64-ab8fbc2c6d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_eHHYokbqGI",
        "outputId": "c69e48e4-980a-4474-ccf1-b14e97df28b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7b8d8d2ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR_ahDk4sLBi"
      },
      "source": [
        "## MedIA_Project2_registered folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "446OARZCrVbz"
      },
      "outputs": [],
      "source": [
        "class MedIARegisteredDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A class used to represent a Medical Image Analysis (MedIA) Registered Dataset.\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    root_dir : str\n",
        "        The path to the root directory containing the patient folders.\n",
        "    transform : callable, optional\n",
        "        An optional transform function to apply to the images.\n",
        "    patient_ids : list\n",
        "        A list of patient IDs used to identify the corresponding patient folders.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(self, root_dir, patient_ids, transform=None):\n",
        "        Initializes the MedIARegisteredDataset object.\n",
        "    __len__(self):\n",
        "        Returns the number of patients in the dataset.\n",
        "    __getitem__(self, idx):\n",
        "        Loads and returns a tuple of the input images (T1w and T2w) and target images (FA and ADC) for the patient\n",
        "        at the given index.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, patient_ids, transform=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        root_dir : str\n",
        "            The path to the root directory containing the patient folders.\n",
        "        patient_ids : list\n",
        "            A list of patient IDs used to identify the corresponding patient folders.\n",
        "        transform : callable, optional\n",
        "            An optional transform function to apply to the images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.patient_ids = patient_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Determines the number of patients in the dataset.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The number of patients in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and returns a tuple of the input images (T1w and T2w) and target images (FA and ADC) for the patient\n",
        "        at the given index.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            The index of the patient.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            A tuple containing two torch.Tensor objects:\n",
        "            - input_imgs: a torch.Tensor containing the stacked T1w and T2w images.\n",
        "            - target_imgs: a torch.Tensor containing the stacked FA and ADC images.\n",
        "        \"\"\"\n",
        "        patient_id = self.patient_ids[idx]\n",
        "        patient_folder = os.path.join(self.root_dir, patient_id)\n",
        "\n",
        "        # Load the input images (T1w and T2w)\n",
        "        t1w_path = os.path.join(patient_folder, 'T1w_1mm.nii.gz')\n",
        "        t2w_path = os.path.join(patient_folder, 'T2w_1mm_noalign.nii.gz')\n",
        "        t1w_img = nib.load(t1w_path).get_fdata()\n",
        "        t2w_img = nib.load(t2w_path).get_fdata()\n",
        "\n",
        "        # Stack the input images along the channel dimension\n",
        "        input_imgs = np.stack([t1w_img, t2w_img], axis=0)\n",
        "\n",
        "        # Load the target images (FA and ADC)\n",
        "        fa_path = os.path.join(patient_folder, 'FA_deformed.nii.gz')\n",
        "        adc_path = os.path.join(patient_folder, 'ADC_deformed.nii.gz')\n",
        "        fa_img = nib.load(fa_path).get_fdata()\n",
        "        adc_img = nib.load(adc_path).get_fdata()\n",
        "\n",
        "        # Stack the target images along the channel dimension\n",
        "        target_imgs = np.stack([fa_img, adc_img], axis=0)\n",
        "\n",
        "        if self.transform:\n",
        "            input_imgs, target_imgs = self.transform(input_imgs, target_imgs)\n",
        "\n",
        "        return torch.tensor(input_imgs).float(), torch.tensor(target_imgs).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOcyu8aozqIE"
      },
      "outputs": [],
      "source": [
        "#root_dir = r\"G:\\My Drive\\Project 2 Neuroimage Synthesis\\MedIA_Project2_registered\"\n",
        "root_dir = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/MedIA_Project2_registered\"\n",
        "\n",
        "# Define patient_ids here\n",
        "patient_ids = [f\"{i:03d}\" for i in range(1, 201) if i != 163]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f1s7ZBsnN2"
      },
      "source": [
        "## split the patient IDs into training, validation, and testing sets (80/10/10 split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRWB5h_issZt"
      },
      "outputs": [],
      "source": [
        "# #patient_ids = [str(i).zfill(3) for i in range(1, 201) if i != 163]\n",
        "rng = np.random.default_rng(seed)\n",
        "rng.shuffle(patient_ids)\n",
        "\n",
        "train_size = int(0.8 * len(patient_ids))\n",
        "val_size = int(0.1 * len(patient_ids))\n",
        "test_size = len(patient_ids) - train_size - val_size\n",
        "\n",
        "train_ids = patient_ids[:train_size]\n",
        "val_ids = patient_ids[train_size:train_size+val_size]\n",
        "test_ids = patient_ids[train_size+val_size:]\n",
        "\n",
        "# separate instances of the dataset for each split\n",
        "train_dataset = MedIARegisteredDataset(root_dir, patient_ids=train_ids)\n",
        "val_dataset = MedIARegisteredDataset(root_dir, patient_ids=val_ids)\n",
        "test_dataset = MedIARegisteredDataset(root_dir, patient_ids=test_ids)\n",
        "\n",
        "# create data loaders for each dataset\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YErk6lRVqiZk"
      },
      "source": [
        "## U-Net with Residual Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5PnE4kQI9-5"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Residual Block in the neural network architecture.\n",
        "    A Residual Block allows the model to learn an identity function\n",
        "    which improves the gradient flow and thus eases the training of deeper models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        Number of input channels to the residual block.\n",
        "    out_channels : int\n",
        "        Number of output channels from the residual block.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    conv1 : torch.nn.Conv3d\n",
        "        The convolution layer in the residual block.\n",
        "    relu1 : torch.nn.ReLU\n",
        "        The ReLU activation function in the residual block.\n",
        "    conv_skip : torch.nn.Conv3d\n",
        "        The convolution layer used in the skip connection when the\n",
        "        number of input and output channels are not equal.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU(inplace=False)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.conv_skip = nn.Conv3d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "        else:\n",
        "            self.conv_skip = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the Residual Block.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            The input tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : torch.Tensor\n",
        "            The output tensor after passing through the Residual Block.\n",
        "        \"\"\"\n",
        "\n",
        "        identity = x\n",
        "        out = self.relu1(self.conv1(x))\n",
        "\n",
        "        if self.conv_skip is not None:\n",
        "            identity = self.conv_skip(identity)\n",
        "\n",
        "        out = out + identity\n",
        "        out = self.relu1(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model instance\n",
        "model = ResidualBlock(in_channels=3, out_channels=16)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "x = torch.randn(1, 3, 32, 32, 32)\n",
        "\n",
        "# Export the model to an ONNX file\n",
        "torch.onnx.export(model, x, \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/resblock.onnx\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMwibcCtkZ-B",
        "outputId": "de759c9c-cb07-4db3-f36e-fb12435edbef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_zD42Lu_tcU"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the implementation of U-Net architecture with residual blocks,\n",
        "    which is a popular architecture for semantic segmentation tasks in the field of\n",
        "    medical imaging, and other related fields.\n",
        "\n",
        "    The U-Net architecture consists of a contracting path (encoder) to capture context and a\n",
        "    symmetric expanding path (decoder) that enables precise localization. The architecture of\n",
        "    U-Net is designed to seamlessly work with very little data and to yield more precise segmentations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        Number of input channels to the U-Net.\n",
        "    out_channels : int\n",
        "        Number of output channels from the U-Net.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    enc1, enc2, enc3, enc4 : ResidualBlock\n",
        "        The Residual Blocks in the contracting (encoder) path.\n",
        "    middle : ResidualBlock\n",
        "        The middle Residual Block.\n",
        "    up4, up3, up2, up1 : torch.nn.ConvTranspose3d\n",
        "        The ConvTranspose3d layers in the expanding (decoder) path.\n",
        "    dec4, dec3, dec2, dec1 : ResidualBlock\n",
        "        The Residual Blocks in the expanding (decoder) path.\n",
        "    pool : torch.nn.MaxPool3d\n",
        "        The MaxPool3d layer.\n",
        "    output : torch.nn.Conv3d\n",
        "        The final Conv3d layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.enc1 = ResidualBlock(in_channels, 32)\n",
        "        self.enc2 = ResidualBlock(32, 64)\n",
        "        self.enc3 = ResidualBlock(64, 128)\n",
        "        self.enc4 = ResidualBlock(128, 256)\n",
        "\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.middle = ResidualBlock(256, 512)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec4 = ResidualBlock(512, 256)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec3 = ResidualBlock(256, 128)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec2 = ResidualBlock(128, 64)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec1 = ResidualBlock(64, 32)\n",
        "\n",
        "        self.output = nn.Conv3d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the U-Net.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            The input tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output : torch.Tensor\n",
        "            The output tensor after passing through the U-Net.\n",
        "        \"\"\"\n",
        "\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        middle = self.middle(self.pool(enc4))\n",
        "\n",
        "        up4 = self.up_concat(self.up4, enc4, middle)\n",
        "        dec4 = self.dec4(up4)\n",
        "\n",
        "        up3 = self.up_concat(self.up3, enc3, dec4)\n",
        "        dec3 = self.dec3(up3)\n",
        "\n",
        "        up2 = self.up_concat(self.up2, enc2, dec3)\n",
        "        dec2 = self.dec2(up2)\n",
        "\n",
        "        up1 = self.up_concat(self.up1, enc1, dec2)\n",
        "        dec1 = self.dec1(up1)\n",
        "\n",
        "        output = self.output(dec1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def up_concat(self, up, encode, decode):\n",
        "        \"\"\"\n",
        "        Concatenates the upsampled tensor and the encoded tensor in the expanding (decoder) path.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        up : torch.nn.ConvTranspose3d\n",
        "            The ConvTranspose3d layer.\n",
        "        encode : torch.Tensor\n",
        "            The tensor from the encoder path.\n",
        "        decode : torch.Tensor\n",
        "            The tensor from the decoder path.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        merge : torch.Tensor\n",
        "            The merged tensor after concatenation.\n",
        "        \"\"\"\n",
        "        up = up(decode)\n",
        "        up = F.interpolate(up, size=encode.size()[2:], mode='trilinear', align_corners=False)\n",
        "        merge = torch.cat([encode, up], dim=1)\n",
        "        return merge\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model instance\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "x = torch.randn(1, 3, 32, 32, 32)\n",
        "\n",
        "# Export the model to an ONNX file\n",
        "torch.onnx.export(model, x, \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/unet_residual.onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU1_Lzqjj2bg",
        "outputId": "d0fbbac8-c661-4e59-9d17-0181ef3f556c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPHeeXiPt9dZ"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u5KlE7Ydc8P"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model\n",
        "model = UNet(2, 2).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "patience = 10\n",
        "early_stopping_counter = 0\n",
        "#model_save_path = \"model.pt\"\n",
        "# model_save_path = r\"G:\\My Drive\\Project 2 Neuroimage Synthesis\\model.pt\"\n",
        "model_save_path = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/residual.pt\"\n",
        "best_validation_loss = float('inf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DePsGVsYt_PE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033679f4-60bf-494c-a377-0e5540f561ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 25.6976, Val Loss: 3.2793, Epoch Duration: 629.0614 seconds, Total Time: 629.0619 seconds\n",
            "Epoch 2/100, Train Loss: 2.4373, Val Loss: 1.8639, Epoch Duration: 140.6804 seconds, Total Time: 770.6237 seconds\n",
            "Epoch 3/100, Train Loss: 1.5573, Val Loss: 1.3050, Epoch Duration: 140.9899 seconds, Total Time: 911.7305 seconds\n",
            "Epoch 4/100, Train Loss: 1.1821, Val Loss: 1.0169, Epoch Duration: 141.1831 seconds, Total Time: 1053.0261 seconds\n",
            "Epoch 5/100, Train Loss: 0.9275, Val Loss: 0.8019, Epoch Duration: 141.1200 seconds, Total Time: 1194.2579 seconds\n",
            "Epoch 6/100, Train Loss: 0.7254, Val Loss: 0.6193, Epoch Duration: 141.6064 seconds, Total Time: 1335.9769 seconds\n",
            "Epoch 7/100, Train Loss: 0.5766, Val Loss: 0.5081, Epoch Duration: 141.4074 seconds, Total Time: 1477.5008 seconds\n",
            "Epoch 8/100, Train Loss: 0.4891, Val Loss: 0.4452, Epoch Duration: 141.3674 seconds, Total Time: 1618.9824 seconds\n",
            "Epoch 9/100, Train Loss: 0.4253, Val Loss: 0.3868, Epoch Duration: 141.6119 seconds, Total Time: 1760.7111 seconds\n",
            "Epoch 10/100, Train Loss: 0.3754, Val Loss: 0.3464, Epoch Duration: 141.2532 seconds, Total Time: 1902.0879 seconds\n",
            "Epoch 11/100, Train Loss: 0.3350, Val Loss: 0.3095, Epoch Duration: 141.4989 seconds, Total Time: 2043.6984 seconds\n",
            "Epoch 12/100, Train Loss: 0.3053, Val Loss: 0.2789, Epoch Duration: 141.3519 seconds, Total Time: 2185.1580 seconds\n",
            "Epoch 13/100, Train Loss: 0.2773, Val Loss: 0.2569, Epoch Duration: 141.1868 seconds, Total Time: 2326.4589 seconds\n",
            "Epoch 14/100, Train Loss: 0.2751, Val Loss: 0.4019, Epoch Duration: 141.8762 seconds, Total Time: 2468.4459 seconds\n",
            "Epoch 15/100, Train Loss: 0.2489, Val Loss: 0.2278, Epoch Duration: 141.4103 seconds, Total Time: 2609.8565 seconds\n",
            "Epoch 16/100, Train Loss: 0.2271, Val Loss: 0.2125, Epoch Duration: 141.5186 seconds, Total Time: 2751.4869 seconds\n",
            "Epoch 17/100, Train Loss: 0.2127, Val Loss: 0.1997, Epoch Duration: 141.6109 seconds, Total Time: 2893.2092 seconds\n",
            "Epoch 18/100, Train Loss: 0.2060, Val Loss: 0.2004, Epoch Duration: 141.5473 seconds, Total Time: 3034.8734 seconds\n",
            "Epoch 19/100, Train Loss: 0.2019, Val Loss: 0.2365, Epoch Duration: 141.7948 seconds, Total Time: 3176.6686 seconds\n",
            "Epoch 20/100, Train Loss: 0.1950, Val Loss: 0.2165, Epoch Duration: 141.9122 seconds, Total Time: 3318.5817 seconds\n",
            "Epoch 21/100, Train Loss: 0.2171, Val Loss: 0.1911, Epoch Duration: 141.6839 seconds, Total Time: 3460.2659 seconds\n",
            "Epoch 22/100, Train Loss: 0.1792, Val Loss: 0.1633, Epoch Duration: 141.2102 seconds, Total Time: 3601.5901 seconds\n",
            "Epoch 23/100, Train Loss: 0.1648, Val Loss: 0.1598, Epoch Duration: 141.2742 seconds, Total Time: 3742.9831 seconds\n",
            "Epoch 24/100, Train Loss: 0.1592, Val Loss: 0.1631, Epoch Duration: 140.1914 seconds, Total Time: 3883.2878 seconds\n",
            "Epoch 25/100, Train Loss: 0.1531, Val Loss: 0.1477, Epoch Duration: 140.6669 seconds, Total Time: 4023.9549 seconds\n",
            "Epoch 26/100, Train Loss: 0.1494, Val Loss: 0.1553, Epoch Duration: 142.2227 seconds, Total Time: 4166.2918 seconds\n",
            "Epoch 27/100, Train Loss: 0.1534, Val Loss: 0.1375, Epoch Duration: 141.8947 seconds, Total Time: 4308.1868 seconds\n",
            "Epoch 28/100, Train Loss: 0.1418, Val Loss: 0.1321, Epoch Duration: 141.4812 seconds, Total Time: 4449.7804 seconds\n",
            "Epoch 29/100, Train Loss: 0.1334, Val Loss: 0.1248, Epoch Duration: 141.5528 seconds, Total Time: 4591.4470 seconds\n",
            "Epoch 30/100, Train Loss: 0.1737, Val Loss: 0.1338, Epoch Duration: 141.1139 seconds, Total Time: 4732.6825 seconds\n",
            "Epoch 31/100, Train Loss: 0.1266, Val Loss: 0.1184, Epoch Duration: 141.7459 seconds, Total Time: 4874.4291 seconds\n",
            "Epoch 32/100, Train Loss: 0.1199, Val Loss: 0.1140, Epoch Duration: 141.6496 seconds, Total Time: 5016.1916 seconds\n",
            "Epoch 33/100, Train Loss: 0.1179, Val Loss: 0.1132, Epoch Duration: 141.6980 seconds, Total Time: 5158.0092 seconds\n",
            "Epoch 34/100, Train Loss: 0.1172, Val Loss: 0.1110, Epoch Duration: 141.3760 seconds, Total Time: 5299.4950 seconds\n",
            "Epoch 35/100, Train Loss: 0.1161, Val Loss: 0.1908, Epoch Duration: 141.7311 seconds, Total Time: 5441.3390 seconds\n",
            "Epoch 36/100, Train Loss: 0.1138, Val Loss: 0.0999, Epoch Duration: 142.0081 seconds, Total Time: 5583.3475 seconds\n",
            "Epoch 37/100, Train Loss: 0.1064, Val Loss: 0.0967, Epoch Duration: 141.5596 seconds, Total Time: 5725.0182 seconds\n",
            "Epoch 38/100, Train Loss: 0.0994, Val Loss: 0.0966, Epoch Duration: 141.3685 seconds, Total Time: 5866.4987 seconds\n",
            "Epoch 39/100, Train Loss: 0.0950, Val Loss: 0.0903, Epoch Duration: 141.6019 seconds, Total Time: 6008.2162 seconds\n",
            "Epoch 40/100, Train Loss: 0.0917, Val Loss: 0.0881, Epoch Duration: 141.1195 seconds, Total Time: 6149.4505 seconds\n",
            "Epoch 41/100, Train Loss: 0.0912, Val Loss: 0.0865, Epoch Duration: 141.4723 seconds, Total Time: 6291.0360 seconds\n",
            "Epoch 42/100, Train Loss: 0.0941, Val Loss: 0.0902, Epoch Duration: 142.5122 seconds, Total Time: 6433.6601 seconds\n",
            "Epoch 43/100, Train Loss: 0.1265, Val Loss: 0.0803, Epoch Duration: 142.0658 seconds, Total Time: 6575.7265 seconds\n",
            "Epoch 44/100, Train Loss: 0.0828, Val Loss: 0.0758, Epoch Duration: 141.9483 seconds, Total Time: 6717.7842 seconds\n",
            "Epoch 45/100, Train Loss: 0.0799, Val Loss: 0.0897, Epoch Duration: 142.1201 seconds, Total Time: 6860.0171 seconds\n",
            "Epoch 46/100, Train Loss: 0.0832, Val Loss: 0.0808, Epoch Duration: 141.6852 seconds, Total Time: 7001.7033 seconds\n",
            "Epoch 47/100, Train Loss: 0.0804, Val Loss: 0.0719, Epoch Duration: 141.5550 seconds, Total Time: 7143.2593 seconds\n",
            "Epoch 48/100, Train Loss: 0.0735, Val Loss: 0.0844, Epoch Duration: 141.6540 seconds, Total Time: 7285.0279 seconds\n",
            "Epoch 49/100, Train Loss: 0.0817, Val Loss: 0.0737, Epoch Duration: 141.4266 seconds, Total Time: 7426.4551 seconds\n",
            "Epoch 50/100, Train Loss: 0.0690, Val Loss: 0.0822, Epoch Duration: 141.8298 seconds, Total Time: 7568.2852 seconds\n",
            "Epoch 51/100, Train Loss: 0.0679, Val Loss: 0.0568, Epoch Duration: 141.6490 seconds, Total Time: 7709.9345 seconds\n",
            "Epoch 52/100, Train Loss: 0.0621, Val Loss: 0.0740, Epoch Duration: 141.5327 seconds, Total Time: 7851.5852 seconds\n",
            "Epoch 53/100, Train Loss: 0.0704, Val Loss: 0.0565, Epoch Duration: 141.7231 seconds, Total Time: 7993.3092 seconds\n",
            "Epoch 54/100, Train Loss: 0.0582, Val Loss: 0.0569, Epoch Duration: 141.5582 seconds, Total Time: 8134.9853 seconds\n",
            "Epoch 55/100, Train Loss: 0.0561, Val Loss: 0.0494, Epoch Duration: 141.5269 seconds, Total Time: 8276.5129 seconds\n",
            "Epoch 56/100, Train Loss: 0.0629, Val Loss: 0.0492, Epoch Duration: 141.6576 seconds, Total Time: 8418.2864 seconds\n",
            "Epoch 57/100, Train Loss: 0.0557, Val Loss: 0.1963, Epoch Duration: 141.5239 seconds, Total Time: 8559.9253 seconds\n",
            "Epoch 58/100, Train Loss: 0.0717, Val Loss: 0.0519, Epoch Duration: 141.5098 seconds, Total Time: 8701.4354 seconds\n",
            "Epoch 59/100, Train Loss: 0.0587, Val Loss: 0.0564, Epoch Duration: 141.8244 seconds, Total Time: 8843.2606 seconds\n",
            "Epoch 60/100, Train Loss: 0.0504, Val Loss: 0.0438, Epoch Duration: 141.7154 seconds, Total Time: 8984.9763 seconds\n",
            "Epoch 61/100, Train Loss: 0.0463, Val Loss: 0.0493, Epoch Duration: 141.5183 seconds, Total Time: 9126.6069 seconds\n",
            "Epoch 62/100, Train Loss: 0.0481, Val Loss: 0.0641, Epoch Duration: 142.0482 seconds, Total Time: 9268.6556 seconds\n",
            "Epoch 63/100, Train Loss: 0.0876, Val Loss: 0.3000, Epoch Duration: 141.6156 seconds, Total Time: 9410.2720 seconds\n",
            "Epoch 64/100, Train Loss: 0.1007, Val Loss: 0.0422, Epoch Duration: 141.2123 seconds, Total Time: 9551.4850 seconds\n",
            "Epoch 65/100, Train Loss: 0.0433, Val Loss: 0.0454, Epoch Duration: 141.4783 seconds, Total Time: 9693.0794 seconds\n",
            "Epoch 66/100, Train Loss: 0.0431, Val Loss: 0.0391, Epoch Duration: 141.7958 seconds, Total Time: 9834.8756 seconds\n",
            "Epoch 67/100, Train Loss: 0.0409, Val Loss: 0.0370, Epoch Duration: 141.5945 seconds, Total Time: 9976.5894 seconds\n",
            "Epoch 68/100, Train Loss: 0.0393, Val Loss: 0.0371, Epoch Duration: 141.5729 seconds, Total Time: 10118.2762 seconds\n",
            "Epoch 69/100, Train Loss: 0.0390, Val Loss: 0.0375, Epoch Duration: 141.3302 seconds, Total Time: 10259.6067 seconds\n",
            "Epoch 70/100, Train Loss: 0.0762, Val Loss: 0.0489, Epoch Duration: 141.5178 seconds, Total Time: 10401.1249 seconds\n",
            "Epoch 71/100, Train Loss: 0.0399, Val Loss: 0.0413, Epoch Duration: 141.5306 seconds, Total Time: 10542.6558 seconds\n",
            "Epoch 72/100, Train Loss: 0.0405, Val Loss: 0.0337, Epoch Duration: 141.5057 seconds, Total Time: 10684.1618 seconds\n",
            "Epoch 73/100, Train Loss: 0.0366, Val Loss: 0.0345, Epoch Duration: 141.2615 seconds, Total Time: 10825.5392 seconds\n",
            "Epoch 74/100, Train Loss: 0.0486, Val Loss: 0.0380, Epoch Duration: 141.8730 seconds, Total Time: 10967.4127 seconds\n",
            "Epoch 75/100, Train Loss: 0.0408, Val Loss: 0.0549, Epoch Duration: 141.7529 seconds, Total Time: 11109.1660 seconds\n",
            "Epoch 76/100, Train Loss: 0.0396, Val Loss: 0.0320, Epoch Duration: 143.3770 seconds, Total Time: 11252.5439 seconds\n",
            "Epoch 77/100, Train Loss: 0.0384, Val Loss: 0.0307, Epoch Duration: 142.4714 seconds, Total Time: 11395.1362 seconds\n",
            "Epoch 78/100, Train Loss: 0.0399, Val Loss: 0.0299, Epoch Duration: 142.6150 seconds, Total Time: 11537.8685 seconds\n",
            "Epoch 79/100, Train Loss: 0.0326, Val Loss: 0.0381, Epoch Duration: 141.6886 seconds, Total Time: 11679.6715 seconds\n",
            "Epoch 80/100, Train Loss: 0.0313, Val Loss: 0.0298, Epoch Duration: 141.5889 seconds, Total Time: 11821.2607 seconds\n",
            "Epoch 81/100, Train Loss: 0.0318, Val Loss: 0.0281, Epoch Duration: 142.3113 seconds, Total Time: 11963.6869 seconds\n",
            "Epoch 82/100, Train Loss: 0.0321, Val Loss: 0.0300, Epoch Duration: 142.2891 seconds, Total Time: 12106.0929 seconds\n",
            "Epoch 83/100, Train Loss: 0.0453, Val Loss: 0.0544, Epoch Duration: 142.0900 seconds, Total Time: 12248.1840 seconds\n",
            "Epoch 84/100, Train Loss: 0.0384, Val Loss: 0.0333, Epoch Duration: 142.4374 seconds, Total Time: 12390.6221 seconds\n",
            "Epoch 85/100, Train Loss: 0.0345, Val Loss: 0.0259, Epoch Duration: 142.6633 seconds, Total Time: 12533.2868 seconds\n",
            "Epoch 86/100, Train Loss: 0.0350, Val Loss: 0.0361, Epoch Duration: 141.5905 seconds, Total Time: 12674.9933 seconds\n",
            "Epoch 87/100, Train Loss: 0.0363, Val Loss: 0.0254, Epoch Duration: 141.4396 seconds, Total Time: 12816.4332 seconds\n",
            "Epoch 88/100, Train Loss: 0.0291, Val Loss: 0.0385, Epoch Duration: 141.3762 seconds, Total Time: 12957.9199 seconds\n",
            "Epoch 89/100, Train Loss: 0.0283, Val Loss: 0.0253, Epoch Duration: 141.6379 seconds, Total Time: 13099.5581 seconds\n",
            "Epoch 90/100, Train Loss: 0.0393, Val Loss: 0.1892, Epoch Duration: 141.6163 seconds, Total Time: 13241.2891 seconds\n",
            "Epoch 91/100, Train Loss: 0.0497, Val Loss: 0.0515, Epoch Duration: 141.8806 seconds, Total Time: 13383.1700 seconds\n",
            "Epoch 92/100, Train Loss: 0.0368, Val Loss: 0.0592, Epoch Duration: 140.7193 seconds, Total Time: 13523.8896 seconds\n",
            "Epoch 93/100, Train Loss: 0.0350, Val Loss: 0.0310, Epoch Duration: 138.0139 seconds, Total Time: 13661.9038 seconds\n",
            "Epoch 94/100, Train Loss: 0.0330, Val Loss: 0.0244, Epoch Duration: 138.1795 seconds, Total Time: 13800.0837 seconds\n",
            "Epoch 95/100, Train Loss: 0.0257, Val Loss: 0.0247, Epoch Duration: 138.5291 seconds, Total Time: 13938.7287 seconds\n",
            "Epoch 96/100, Train Loss: 0.0304, Val Loss: 0.0215, Epoch Duration: 138.0370 seconds, Total Time: 14076.7660 seconds\n",
            "Epoch 97/100, Train Loss: 0.0298, Val Loss: 0.0257, Epoch Duration: 138.0312 seconds, Total Time: 14214.9103 seconds\n",
            "Epoch 98/100, Train Loss: 0.0306, Val Loss: 0.0330, Epoch Duration: 138.1646 seconds, Total Time: 14353.0752 seconds\n",
            "Epoch 99/100, Train Loss: 0.0317, Val Loss: 0.0420, Epoch Duration: 138.3269 seconds, Total Time: 14491.4027 seconds\n",
            "Epoch 100/100, Train Loss: 0.0359, Val Loss: 0.0983, Epoch Duration: 137.9854 seconds, Total Time: 14629.3884 seconds\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "#patience = 10\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    model.train()\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for i, (input_imgs, target_imgs) in enumerate(train_loader):\n",
        "        input_imgs = input_imgs.to(device)\n",
        "        target_imgs = F.interpolate(target_imgs, size=[182, 218, 182], mode='trilinear', align_corners=False)\n",
        "        #target_imgs = F.interpolate(target_imgs, size=(182, 218, 182), mode='trilinear', align_corners=False)\n",
        "        target_imgs = target_imgs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_imgs)\n",
        "        loss = criterion(outputs, target_imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * input_imgs.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = F.interpolate(targets, size=[182, 218, 182], mode='trilinear', align_corners=False)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    total_training_time = time.time() - start_time\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch Duration: {epoch_duration:.4f} seconds, Total Time: {total_training_time:.4f} seconds')\n",
        "\n",
        "    if val_loss < best_validation_loss:\n",
        "        best_validation_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myYHyL3-m5ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7c4945-28d9-4000-f280-83fbe151c12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training time: 15227.0324 seconds\n",
            "Average Epoch Time: 152.27 seconds\n"
          ]
        }
      ],
      "source": [
        "# Print total training time\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f'Total training time: {total_time:.4f} seconds')\n",
        "\n",
        "avg_epoch_time = total_time / num_epochs\n",
        "print(f\"Average Epoch Time: {avg_epoch_time:.2f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model State"
      ],
      "metadata": {
        "id": "7ugK47y9QTlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, \"residual.pt\")\n"
      ],
      "metadata": {
        "id": "J0T9eVvwkY4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMcgQG-dzPZV"
      },
      "source": [
        "## Train/Val Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBeNThFdpoUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "c5797ac0-d575-4649-bbeb-344ef98d92af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training time: 34043.1511 seconds\n"
          ]
        }
      ],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "#plt.savefig(\"/content/drive/My Drive/Project 2 Neuroimage Synthesis/loss_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "# Total training time\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f'Total training time: {total_time:.4f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reload Model"
      ],
      "metadata": {
        "id": "_wwZlQALTGLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m91sgh84TGHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model\n",
        "model = UNet(2, 2).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "bROhrRrC_ubf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "#model_path = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/unet.pt\"\n",
        "\n",
        "model_path = \"/content/drive/My Drive/Project 2 Neuroimage Synthesis/Team2/code/Code - Project 2 Data/residual.pt\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Switch to eval mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gcIyM5-d-HP",
        "outputId": "b4eb1f3b-2aa0-4ec4-f87d-9638f6ddac45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (enc1): ResidualBlock(\n",
              "    (conv1): Conv3d(2, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(2, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (enc2): ResidualBlock(\n",
              "    (conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (enc3): ResidualBlock(\n",
              "    (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (enc4): ResidualBlock(\n",
              "    (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (middle): ResidualBlock(\n",
              "    (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (up4): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec4): ResidualBlock(\n",
              "    (conv1): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (up3): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec3): ResidualBlock(\n",
              "    (conv1): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (up2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec2): ResidualBlock(\n",
              "    (conv1): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (up1): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
              "  (dec1): ResidualBlock(\n",
              "    (conv1): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (relu1): ReLU()\n",
              "    (conv_skip): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              "  )\n",
              "  (output): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg43aZtYuEHE"
      },
      "source": [
        "## Test - Produce Synthesized Maps of FA and ADC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6C0trMP6uNF",
        "outputId": "ff995fef-e460-497f-e9d2-7143070ee69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['103',\n",
              " '177',\n",
              " '140',\n",
              " '184',\n",
              " '042',\n",
              " '111',\n",
              " '023',\n",
              " '020',\n",
              " '165',\n",
              " '065',\n",
              " '048',\n",
              " '114',\n",
              " '141',\n",
              " '183',\n",
              " '015',\n",
              " '142',\n",
              " '196',\n",
              " '037',\n",
              " '098',\n",
              " '195',\n",
              " '137']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import nibabel as nib\n",
        "from nibabel.processing import resample_to_output\n",
        "import csv\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import mean_squared_error as mse"
      ],
      "metadata": {
        "id": "6q-FAlYRAJA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nilearn.image import resample_img\n",
        "\n",
        "# Define the new voxel size for resampling\n",
        "new_voxel_size = (1.25, 1.25, 1.25)\n",
        "\n",
        "successful_patients = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (input_imgs, target_imgs) in enumerate(test_loader):\n",
        "        patient_id = test_ids[i]\n",
        "        print(f\"Processing patient {patient_id}\")\n",
        "\n",
        "        # Save the original images for visualization\n",
        "        original_input_imgs = input_imgs.clone()\n",
        "\n",
        "        # Move tensors to the device\n",
        "        input_imgs = input_imgs.to(device)\n",
        "        target_imgs = target_imgs.to(device)\n",
        "\n",
        "        # Generate output maps from input images\n",
        "        outputs = model(input_imgs)\n",
        "\n",
        "        # Check the number of outputs\n",
        "        if len(outputs) == 2:\n",
        "            # Unpack the outputs\n",
        "            fake_fa, fake_adc = outputs\n",
        "\n",
        "            # Convert tensors to Nifti images\n",
        "            fake_fa_nifti = nib.Nifti1Image(fake_fa[0].cpu().numpy(), np.eye(4))\n",
        "            fake_adc_nifti = nib.Nifti1Image(fake_adc[0].cpu().numpy(), np.eye(4))\n",
        "\n",
        "            # Resample the images to the new voxel size\n",
        "            fake_fa_nifti = resample_img(fake_fa_nifti, target_affine=np.diag(new_voxel_size))\n",
        "            fake_adc_nifti = resample_img(fake_adc_nifti, target_affine=np.diag(new_voxel_size))\n",
        "\n",
        "            # Save the synthesized FA and ADC maps\n",
        "            patient_id = test_ids[i]\n",
        "            patient_folder = os.path.join(root_dir, patient_id)\n",
        "\n",
        "            fa_syn_path = os.path.join(patient_folder, 'FA_syn.nii.gz')\n",
        "            adc_syn_path = os.path.join(patient_folder, 'ADC_syn.nii.gz')\n",
        "\n",
        "            nib.save(fake_fa_nifti, fa_syn_path)\n",
        "            nib.save(fake_adc_nifti, adc_syn_path)\n",
        "\n",
        "            # Add the successful patient_id to the list\n",
        "            successful_patients.append(patient_id)\n",
        "        elif len(outputs) == 1:\n",
        "            print(f\"Only one output for patient {test_ids[i]}, skipping.\")\n",
        "        else:\n",
        "            print(f\"Unexpected number of outputs for patient {test_ids[i]}, skipping.\")\n",
        "\n",
        "print(\"Successfully saved patients:\", successful_patients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl4HipBqANic",
        "outputId": "2b492eeb-1042-4ad0-98d6-a124eed1a0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing patient 103\n",
            "Processing patient 177\n",
            "Processing patient 140\n",
            "Processing patient 184\n",
            "Processing patient 042\n",
            "Processing patient 111\n",
            "Processing patient 023\n",
            "Processing patient 020\n",
            "Processing patient 165\n",
            "Processing patient 065\n",
            "Processing patient 048\n",
            "Only one output for patient 048, skipping.\n",
            "Successfully saved patients: ['103', '177', '140', '184', '042', '111', '023', '020', '165', '065']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Metrics (MSE, PSNR, SSIM) to a CSV comparing synthesized FA and ADC maps to original FA and ADC for each patient in test set."
      ],
      "metadata": {
        "id": "rBdCdm08AsRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from skimage.metrics import mean_squared_error as mse, peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from nilearn.image import resample_to_img\n",
        "\n"
      ],
      "metadata": {
        "id": "Otro-sDGA3ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfGyPvbXopmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be075b2-5a75-4744-ccbf-ae3e73f89241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE FA: 0.034004120037076396\n",
            "Average PSNR FA: 30.995720191780993\n",
            "Average SSIM FA: 0.6948640114484184\n",
            "Average MSE ADC: 0.005319285478019584\n",
            "Average PSNR ADC: 38.96095239068801\n",
            "Average SSIM ADC: 0.9519492665623541\n",
            "Average MAE FA: 0.07670140275536799\n",
            "Average MAE ADC: 0.013384405978669522\n",
            "Metrics saved to metrics.csv\n"
          ]
        }
      ],
      "source": [
        "# Define the headers for the CSV file\n",
        "headers = ['Patient_ID', 'MSE_FA', 'PSNR_FA', 'SSIM_FA', 'MAE_FA', 'MSE_ADC', 'PSNR_ADC', 'SSIM_ADC', 'MAE_ADC']\n",
        "\n",
        "\n",
        "# Initialize running sums for calculating the averages\n",
        "sum_mse_fa, sum_psnr_fa, sum_ssim_fa = 0, 0, 0\n",
        "sum_mse_adc, sum_psnr_adc, sum_ssim_adc = 0, 0, 0\n",
        "sum_mae_fa, sum_mae_adc = 0, 0\n",
        "\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open('residual_metrics.csv', 'w', newline='') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow(headers)\n",
        "\n",
        "    for patient_id in successful_patients:\n",
        "        patient_folder = os.path.join(root_dir, patient_id)\n",
        "\n",
        "        fa_syn_path = os.path.join(patient_folder, 'FA_syn.nii.gz')\n",
        "        adc_syn_path = os.path.join(patient_folder, 'ADC_syn.nii.gz')\n",
        "        fa_deformed_path = os.path.join(patient_folder, 'FA_deformed.nii.gz')\n",
        "        adc_deformed_path = os.path.join(patient_folder, 'ADC_deformed.nii.gz')\n",
        "\n",
        "        fa_syn_nifti = nib.load(fa_syn_path)\n",
        "        fa_syn = fa_syn_nifti.get_fdata()\n",
        "        adc_syn_nifti = nib.load(adc_syn_path)\n",
        "        adc_syn = adc_syn_nifti.get_fdata()\n",
        "\n",
        "        fa_deformed_nifti = nib.load(fa_deformed_path)\n",
        "        fa_deformed = fa_deformed_nifti.get_fdata()\n",
        "        adc_deformed_nifti = nib.load(adc_deformed_path)\n",
        "        adc_deformed = adc_deformed_nifti.get_fdata()\n",
        "\n",
        "        # Resample the synthesized FA and ADC images to match the deformed FA and ADC images\n",
        "        fa_syn_nifti = resample_to_img(fa_syn_nifti, fa_deformed_nifti)\n",
        "        fa_syn = fa_syn_nifti.get_fdata()\n",
        "        adc_syn_nifti = resample_to_img(adc_syn_nifti, adc_deformed_nifti)\n",
        "        adc_syn = adc_syn_nifti.get_fdata()\n",
        "\n",
        "        mae_fa = mean_absolute_error(fa_syn.flatten(), fa_deformed.flatten())\n",
        "        mse_fa = mse(fa_syn, fa_deformed)\n",
        "        psnr_fa = psnr(fa_syn, fa_deformed, data_range=fa_syn.max() - fa_syn.min())\n",
        "        ssim_fa = ssim(fa_syn, fa_deformed)\n",
        "\n",
        "        mae_adc = mean_absolute_error(adc_syn.flatten(), adc_deformed.flatten())\n",
        "        mse_adc = mse(adc_syn, adc_deformed)\n",
        "        psnr_adc = psnr(adc_syn, adc_deformed, data_range=adc_syn.max() - adc_syn.min())\n",
        "        ssim_adc = ssim(adc_syn, adc_deformed)\n",
        "\n",
        "        # Write the metrics to the CSV file\n",
        "        csv_writer.writerow([patient_id, mse_fa, psnr_fa, ssim_fa, mae_fa, mse_adc, psnr_adc, ssim_adc, mae_adc])\n",
        "\n",
        "        # Update the running sums\n",
        "        sum_mse_fa += mse_fa\n",
        "        sum_psnr_fa += psnr_fa\n",
        "        sum_ssim_fa += ssim_fa\n",
        "        sum_mse_adc += mse_adc\n",
        "        sum_psnr_adc += psnr_adc\n",
        "        sum_ssim_adc += ssim_adc\n",
        "\n",
        "        sum_mae_fa += mae_fa\n",
        "        sum_mae_adc += mae_adc\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the averages\n",
        "num_successful_patients = len(successful_patients)\n",
        "avg_mse_fa = sum_mse_fa / num_successful_patients\n",
        "avg_psnr_fa = sum_psnr_fa / num_successful_patients\n",
        "avg_ssim_fa = sum_ssim_fa / num_successful_patients\n",
        "avg_mse_adc = sum_mse_adc / num_successful_patients\n",
        "avg_psnr_adc = sum_psnr_adc / num_successful_patients\n",
        "avg_ssim_adc = sum_ssim_adc / num_successful_patients\n",
        "avg_mae_fa = sum_mae_fa / num_successful_patients\n",
        "avg_mae_adc = sum_mae_adc / num_successful_patients\n",
        "\n",
        "\n",
        "# Output the averages\n",
        "print(\"Average MSE FA:\", avg_mse_fa)\n",
        "print(\"Average PSNR FA:\", avg_psnr_fa)\n",
        "print(\"Average SSIM FA:\", avg_ssim_fa)\n",
        "print(\"Average MSE ADC:\", avg_mse_adc)\n",
        "print(\"Average PSNR ADC:\", avg_psnr_adc)\n",
        "print(\"Average SSIM ADC:\", avg_ssim_adc)\n",
        "print(\"Average MAE FA:\", avg_mae_fa)\n",
        "print(\"Average MAE ADC:\", avg_mae_adc)\n",
        "\n",
        "\n",
        "print(\"Metrics saved to metrics.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}